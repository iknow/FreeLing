\documentclass[a4paper]{book}
\usepackage{a4wide}
\usepackage{fancyhdr}
\usepackage{graphicx}

\setcounter{tocdepth}{1}

\begin{document}

\begin{titlepage}
\vspace*{7cm}
\begin{center}
{\Large FreeLing User Manual\\[1ex]\large 2.1}\\
\vspace*{1cm}
{\small June 2009}\\
\end{center}
\end{titlepage}

{\newpage{\pagestyle{empty}\cleardoublepage}}
\pagenumbering{roman}

\tableofcontents

{\newpage{\pagestyle{empty}\cleardoublepage}}
\pagenumbering{arabic}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Introduction}

The FreeLing package consists of a library providing language analysis
services (such as morphological analysis, date recognition, PoS
tagging, etc.)

The current version provides tokenizing, sentence splitting,
morphological analysis, NE detection and classification, recognition
of dates/numbers/physical magnitudes/currency/ratios, PoS tagging,
shallow parsing, dependency parsing, WN-based sense annotation, and
coreference resolution. Future versions are expected to improve
performance in existing functionalities, as well as incorporate new
features, such as word sense disambiguation, document classification, etc.

FreeLing is designed to be used as an external library from any
application requiring this kind of services. Nevertheless, a simple
main program is also provided as a basic interface to the library,
which enables the user to analyze text files from the command line.

%..................................................
\section{What is FreeLing}

FreeLing is a developer-oriented library providing language analysis services.
If you want to develop, say, a machine translation system, and you need some
kind of linguistic processing of the source text, your MT application can
call FreeLing modules to do the required analysis.

In the directory src/main/simple\_examples in FreeLing tarball, some
sample programs are provided to illustrate how an application program 
can call the library.

%..................................................
\section{What is NOT FreeLing}

FreeLing is not a user-oriented text analysis tool. That is, it is not designed
to be user friendly, or to output results with a cute image, or in a certain format.

FreeLing results are linguistic analysis in a data structure. Each end-user application
(e.g. anything from a machine translation system to a syntactic-tree drawing interface)
can access those data and process them as needed.

Nevertheless, FreeLing package provides a quite complete application
program ({\tt analyzer}) that enables an end user to obtain the
analysis of a text. See chapter \ref{cap-analyzer} for details.

This program offers a set of options that cover almost all FreeLing capabilities. Nevertheless,
much more advantadge can be taken of FreeLing, and more information can be accessed if you 
call FreeLing from your own application program as described above.

%..................................................
\section{Supported Languages}

The distributed version supports (to different extents)
English, Spanish, Catalan, Galician, Italian, Portuguese, and Welsch.

See the {\tt Linguistic Data} section on FreeLing webpage to find out
about the size of the morphological dictionaries and the source of their data.

 FreeLing also includes WordNet-based sense dictionaries for some of
the covered languages, as well as some knowledge extracted from
WordNet, such as semantic file codes, or hypernymy relationships.
\begin{itemize}
   \item The English sense dictionary is straightforwardly extracted from WN 1.6
     and therefore is distributed under the terms of WN license, as is all 
     knowledge extracted from WN contained in thos package. 

   \item Catalan and Spanish sense dictionaries are extracted from EuroWordNet, and the
     reduced subsets included in this FreeLing package are distributed under GPL.

   See http://wordnet.princeton.edu for details on WordNet, \\
   and http://www.illc.uva.nl/EuroWordNet for more information on EuroWordNet.
   
\end{itemize}

%..................................................
\section{License}

\noindent FreeLing code is licensed under GNU General Public License (GPL). 
  
\noindent The linguistic data are licensed under diverse licenses, depending on their original sources.

\noindent Find the details in the {\tt COPYING} file in the tarball, or in the
  {\tt License} section in FreeLing webpage.

%..................................................
\section{Contributions}
 
  FreeLing was originally written by people in TALP Research 
  Center at Universitat Politecnica de Catalunya (http://talp.upc.es). 

  Spanish and Catalan linguistic data were originally developed 
  by people in CLiC, Centre de Llenguatge i Computacio at Universitat
  de Barcelona (http://clic.fil.ub.es).

  Many people further contributed to by reporting problems, suggesting 
  various improvements, submitting actual code or extending linguistic 
  databases. 

  A detailed list can be found in {\sl Contributions} section at 
  FreeLing webpage ({\tt http://www.lsi.upc.edu/~nlp/freeling}).



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Getting it to work}

%..................................................
\section{Requirements}
\label{sec-requirements}

 To install FreeLing you'll need:

 \begin{itemize}
 \item A typical Linux box with usual development tools:
    \begin{itemize}
    \item bash
    \item make
    \item C++ compiler with basic STL support
    \end{itemize}

 \item Enough hard disk space (about 400Mb)

 \item Some external libraries are required to compile FreeLing:
    \begin{itemize}
    \item {\tt libpcre} (version 4.3 or higher)\\ 
      Perl C Regular
      Expressions. Included in most Linux distributions. Just make
      sure you have installed both development and runtime packages.

      {\sl Orientative package names} (check the package manager in your system):\\
      Ubuntu/Debian: libpcre3-dev libpcre3\\
      OpenSuse/Fedora/Mandriva: libpcre libpcre-devel \\

     \item {\tt libdb} (version 4.1.25 or higher) \\
       Berkeley DB. Included in all Linux distributions. You probably have it
       already installed. Just make sure you have installed both
       runtime and development packages, as well as C++ support:\\

       {\sl Orientative package names} (check the package manager in your system):\\
       Ubuntu/Debian: libdb4.6 libdb4.6-dev libdb4.6++ libdb4.6++-dev\\
       OpenSuse/Fedora/Mandriva: libdb-4.6 libdb-4.6-devel libdb\_cxx-4.6 libdb\_cxx-4.6-devel  \\

     \item {\tt libboost} (version 1.31 or higher) \\
       Boost library. Included in all Linux distributions. You probably {\bf do not} have it
       installed. Make sure to installed both runtime and development packages:\\

       {\sl Orientative package names} (check the package manager in your system):\\
       Ubuntu/Debian: libboost-dev libboost-filesystem libboost-graph\\
       OpenSuse/Fedora/Mandriva: libboost-devel libboost  \\

     \item {\tt libcfg+} (version 0.6.1 or higher) \\
       Configuration file and command-line options management. 
       Probably not included in your linux distribution.
       Available from {\tt http://www.platon.sk/projects/libcfg+/},
       follow installation instructions provided in the libcfg+ package. \\

     \item {\tt Omlet \& Fries} (omlet v.1.0 or higher, fries v.1.1 or higher)\\
        Machine Learning utility libraries, used by Named Entity Classifier.
        These libraries contain linguistic data strucrures, so they are
        required even if you do not plan to use the NEC ability of FreeLing.
        Available from {\tt http://www.lsi.upc.edu/\~{\ }/nlp/omlet+fries}

    \end{itemize}
\end{itemize}

   See details on the installation procedure in section \ref{sec-installation}.

%..................................................
\section{Installation}
\label{sec-installation}

Installation follows standard GNU autoconfigure installation procedures (that is, 
the usual \\
\verb#./configure && make && make install# stuff).

This section provides a detailed guide on how to install FreeLing (and all its
required packages)

\subsection{Install from {\tt .deb} packages}
\label{sec-install-deb}

This installation procedure is the fastest and easiest. 
If you do not plan to modify the code, this is the option you should take.

Runtime libraries needed by FreeLing must be installed beforehand. Package names and organization may vary from one distribution to another. Here are some examples for some of the most usual ones.

\begin{itemize}
\item {\sl Ubuntu (8.04 Hardy, 8.10 Intrepid, 9.04 Jaunty), Debian 5.0.2 Lenny}
\begin{verbatim}
  sudo apt-get install libdb4.6++ libpcre3 libboost-filesystem1.34.1
  sudo dpkg -i FreeLing-2.1.deb
\end{verbatim}

   In the case of Debian, the above commands must be issued as root and without {\tt sudo}.
\end{itemize}


\subsection{Install from {\tt .tar.gz} source packages}
\label{sec-install-tgz}

 Installing from source is slower and harder, but it will work in any Linux box, even if
you have library versions different than those required by the {\tt .deb} package.

\begin{enumerate}

\item {\bf Install development tools}\\
 You'll need to install the C++ compiler:
\begin{verbatim}
  sudo apt-get install build-essential
\end{verbatim}
 (In Debian, use the same command as root, without {\sl sudo}).

\item {\bf Install packaged requirements}

 Many of the required libraries are standard packages in all Linux distributions. 
Just open your favorite software package manager and install them.

  Package names may vary slightly in different
 distributions. See section \ref{sec-requirements} for some hints on
 possible package names.

 As an example, commands to install the packages from command line in
 Ubuntu and Debian are provided, though you can do the same using synaptic or a
 similar manager. 

\begin{itemize}
\item {\sl Install libpcre headers:} \\
 Installing the headers will pull in the binaries, if not already there:
\begin{verbatim}
 sudo apt-get install libpcre3-dev 
\end{verbatim}

\item {\sl Install libdb4 for C++:}\\
 Version doesn't need to be exactly 4.6. Any 4.x available in your distribution will do.
 Installing the headers will pull in the binaries:
\begin{verbatim}
 sudo apt-get install libdb4.6++-dev
\end{verbatim}

\item {\sl Install libboost headers and binaries:}\\ 
In Ubuntu Intrepid, Ubuntu Jaunty, and Debian Lenny, installing this single package, will pull in all
the required others:
\begin{verbatim}
 sudo apt-get install libboost-dev
\end{verbatim}

In Ubuntu Hardy packages are organized slightly different:
\begin{verbatim}
 sudo apt-get install libboost-filesystem-dev libboost-graph-dev
\end{verbatim}
\end{itemize}

\item {\bf Install non-packaged requirements}

\begin{itemize}
\item {\sl Install libcfg+ library}\\
 This library is not usually found packaged in Linux distributions, so you need to install it from source.
\begin{verbatim}
  wget http://platon.sk/projects/download.php?id=57
  tar xzvf libcfg+-0.6.2.tar.gz
  cd libcfg+-0.6.2
  ./configure CPPFLAGS="-fPIC"
  make
  sudo make install
\end{verbatim}

  If you do not have {\tt wget}, you simply can get the package directly from {\tt http://www.platon.sk/projects/libcfg+/}.
\end{itemize}


\item {\bf Install FreeLing}

\begin{itemize}
\item {\sl Install libfries:}
\begin{verbatim}
  tar xzvf libfries-1.1.tar.gz
  ./configure
  make
  sudo make install
\end{verbatim}

\item {\sl Install libomlet:}
\begin{verbatim}
  tar xzvf libomlet-1.0.tar.gz
  ./configure
  make
  sudo make install
\end{verbatim}

 Omlet\&Fries libraries are contained in the files {\tt libomlet.so} and {\tt libfries.so}, installed by default in {\tt /usr/local/lib}. If you're a developer, you need to know that {\tt libfries.so} contains the classes storing linguistic information, which are described in section \ref{ssec-data}.

\item {\sl Install FreeLing itself:}
\begin{verbatim}
  tar xzvf FreeLing-2.1.tar.gz
  ./configure
  make
  sudo make install
\end{verbatim}
\end{itemize}

   Note that if you (or the library package) install some libraries or
   headers in non-standard directories (that is, other than {\tt
     /usr/lib} or {\tt /usr/local/lib} for libraries, or other than
   {\tt /usr/include} or {\tt /usr/local/include} for headers) you may
   need to use the {\tt CPPFLAGS} or {\tt LDFLAGS} variables to
   properly run the {\tt ./configure} script when compiling FreeLing.

    For instance, if you installed BerkeleyDB from a {\tt rpm}
   package, the {\tt db\_cxx.h} header file may be located at {\tt
   /usr/include/db4} instead of the default {\tt /usr/include}.  So,
   if {\tt ./configure} complains about not finding the library
   header, you'll have to specify where to find it, with something like:\\ 
   {\tt ./configure CPPFLAGS='-I/usr/include/db4'} 

 FreeLing library is entirely contained in the file {\tt libmorfo.so}
 installed by default in {\tt /usr/local/lib}.

 Sample program {\tt analyze} is installed in {\tt
   /usr/local/bin}. See sections \ref{sec-execute} and
 \ref{cap-analyzer} for details.
\end{enumerate}


\subsection{Install from {\tt SVN} repositories}
\label{sec-install-svn}

  Installing from the SVN is very similar to installing from source, but you'll have the chance to easily update your FreeLing to the latest development version.

\begin{enumerate}

\item {\bf Install development tools}\\
 You'll need to install the C++ compiler, the GNU autotools, and a SVN client.
\begin{verbatim}
  sudo apt-get install build-essential automake libtool subversion
\end{verbatim}
  
\item {\bf Install packaged and non-packaged requirements}\\ 
  Follow
  the same procedure described in section \ref{sec-install-tgz} for
  these two steps.

\item {\bf Checkout FreeLing sources}\\
\begin{verbatim}
    mkdir mysrc
    cd mysrc
    svn checkout http://devel.cpl.upc.edu/freeling/svn/latest/freeling
    svn checkout http://devel.cpl.upc.edu/freeling/svn/latest/omlet
    svn checkout http://devel.cpl.upc.edu/freeling/svn/latest/fries
\end{verbatim}

\item {\bf Prepare local repositories for compilation}
\begin{verbatim}
    cd fries
    aclocal; libtoolize; autoconf; automake -a
    cd ../omlet
    aclocal; libtoolize; autoconf; automake -a
    cd ../freeling
    aclocal; libtoolize; autoconf; automake -a
    cd ..
\end{verbatim}

\item {\bf Build and install FreeLing}
\begin{verbatim}
    cd fries
    ./configure && make
    sudo make install

    cd ../omlet
    ./configure && make
    sudo make install

    cd ../freeling
    ./configure && make
    sudo make install
\end{verbatim}

\end{enumerate}

\noindent If you keep the svn directories, you will be able to update to the latest version at any moment:
\begin{verbatim}
    cd freeling
    svn update
    ./configure && make
    sudo make install
\end{verbatim}
  Obviously, you can also update {\tt fries} or {\tt omlet} versions in the same way.

%..................................................
\section{Executing}
\label{sec-execute}

  FreeLing is a library, which means that it not a final-user oriented
  executable program, but a tool to develop new programs which may
  require linguistic analysis services.

  Nevertheless, a sample main program is included in the package for
 those who just want a text analyzer. This program may 
 be adapted to fit your needs (e.g. customized input/output formats).
  
  The usage and options of this main program is described in chapter \ref{cap-analyzer}.
  
  Please take into account that this program is only a friendly
  interface to demonstrate FreeLings abilities, but that there are
  many other potential usages of FreeLing.  

  Thus, the question is not {\sl why this program doesn't offer
    functionality X?}, {\sl why it doesn't output information Y?}, or
  {\sl why it doesn't present results in format Z?}, but {\sl How can
    I use FreeLing to write a program that does exactly what I need?}.
    
  In the directory {\tt src/main/simple\_examples} in the tarball, you
 can find simpler sample programs that illustrate how to call the library, 
 and that can be used as a starting point to develop your own application.

%..................................................
\section{Porting to other platforms}

The FreeLing library is entirely written in C++, so it should be
possible to compile it on non-unix platforms with a reasonable
effort (additional pcre/db/cfg+ libraries porting might be 
required also...). 

Success have been reported on compiling FreeLing on MacOS, as well as on
MS-Windows using cygwin (http://www.cygwin.com/).

See the {\tt Installing} section and the Forum in FreeLing webpage for details.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Analysis Modules}

 In this chapter, each of the modules in FreeLing are described. 

 A typical module receives a list of sentences, and enriches its words with new analysis.

 Usually, when the module is instantiated, it receives as a parameter
 the name of a file where the information and/or parameters needed by
 the module is stored (e.g. a dictionary file for the dictionary
 search module, or a CFG grammar for a parser module).
 
 Most modules are language-independent, that is, if the provided file
 contains data for another language, the same module will be able to
 process that language.

  If an application needs to process more than one language, it can 
 instantiate the needed modules for each language, just calling the 
 constructors with different data files as a parameter.

%..................................................
\section{Tokenizer Module}
\label{file-tok}

  The first module in the chain is the tokenizer. It converts plain text to a vector of {\tt word} objects, according to a set of tokenization rules.

  Tokenization rules are regular expressions that are matched against the beggining of the text line being processed. The first matching rule is used to extract the token, the matching substring is deleted from the line, and the process is repeated until the line is empty.

  The API of the tokenizer module is the following:
\begin{verbatim}
class tokenizer {
  public:
    /// Constructor, receives the name of the file with tokenization rules
    tokenizer(const std::string &);

    /// tokenize string with default options
    std::list<word> tokenize(const std::string &);

    /// tokenize string with default options, accumulating byte-offset of words
    std::list<word> tokenize(const std::string &, unsigned long &);
};
\end{verbatim}

  That is, once created, the tokenizer module receives plain text in a string, tokenizes it, and returns a list of {\tt word} objects corresponding to the created tokens

\subsection{Tokenizer Rules File}

  The tokenizer rules file is divided in three sections
  \verb#<Macros>#, \verb#<RegExps># and \verb#<Abbreviations>#.  Each
  section is closed by \verb#</Macros>#, \verb#</RegExps># and
  \verb#</Abbreviations># tags respectively.
  
   The \verb#<Macros># section allows the user to define regexp macros
   that will be used later in the rules. Macros are defined with a name and
   a Perl regexp.\\
   E.g. {\tt ALPHA  [A-Za-z]}

    The \verb#<RegExps># section defines the tokenization
   rules. Previously defined macros may be referred to with their name
   in curly brackets. \\
   E.g. \verb#*ABREVIATIONS1  0  ((\{ALPHA\}+\.)+)(?!\.\.)#

   Rules are regular expressions, and are applied in the order of definition. 
   The first rule matching the {\em beginning} of the line is applied,
   a token is built, and the rest of the rules are ignored.
   The process is repeated until the line has been completely processed.

   The format of each rule is:
  \begin{itemize}
    \item The first field in the rule is the rule name. If it starts
     with a {\tt *}, the RegExp will only produce a token if the 
     match is found in abbreviation list (\verb#<Abbreviations># section).
     Apart from that, the rule name is only for informative/readability purposes.
    \item The second field in the rule is the substring to form the token/s with.
     It may be 0 (the match of the whole expression) or any number
     from 1 to the number of substrings (up to 9). A token will be
     created for each substring from 1 to the specified value.
    \item The third field is the regexp to match against the input.
      line. Any Perl regexp convention may be used.
  \end{itemize}

  The \verb#<Abbreviations># section defines common abbreviations (one
  per line) that must not be separated of their following dot
  (e.g. {\tt etc.}, {\tt mrs.}). They must be lowercased, even if
  they are expected to appear uppercased in the text.

%..................................................
\section{Splitter Module}
\label{file-split}

  The splitter module receives lists of {\tt word} objects (either produced by the 
tokenizer or by any other means in the calling application) and buffers them 
until a sentence boundary is detected. 
  Then, a list of {\tt sentence} objects is returned. 

  The buffer of the splitter may retain part of the tokens if the
  given list didn't end with a clear sentence boundary.  The caller
  application can sumbit further token lists to be added, or request
  the splitter to flush the buffer.

 The API for the splitter class is:
\begin{verbatim}
class splitter {
   public:
      /// Constructor. Receives a file with the desired options
      splitter(const std::string &);

      /// Add list of words to the buffer, and return complete sentences 
      /// that can be build.
      /// The boolean states if a buffer flush has to be forced (true) or
      /// some words may remain in the buffer (false) if the splitter 
      /// wants to wait to see what is coming next.
      std::list<sentence> split(const std::list<word> &, bool);
};
\end{verbatim}

\subsection{Splitter Options File}

The splitter options file contains four sections: \verb#<General>#, \verb#<Markers>#,
\verb#<SentenceEnd>#, and \verb#<SentenceStart>#.

The \verb#<General># section contains general options for the
splitter: Namely, {\tt AllowBetweenMarkers} and {\tt MaxLines}
options. The former may take values 1 or 0 (on/off). The
later may be any integer. An example of the  \verb#<General># section is:
\begin{verbatim}
<General>
AllowBetweenMarkers 0
MaxLines 0
</General>
\end{verbatim}

If {\tt AllowBetweenMarkers} is off ({\tt 0}), a sentence split will never be
introduced inside a pair of parenthesis-like markers, which is useful
to prevent splitting in sentences such as {\em ``I hate'' (Mary
said. Angryly.) ``apple pie''.}
 If this option is on ({\tt 1}), a sentence end is allowed to be 
introduced inside such a pair.

{\tt MaxLines} states how many text lines are read before forcing
a sentence split inside parenthesis-like markers (this option is intended to
avoid memory fillups in case the markers are not properly closed in
the text).  A value of zero means ``Never split, I'll risk to a memory fillup''.
 Obviously, this option is only effective if {\tt AllowBetweenMarkers} is on.

The \verb#<Markers># section lists the pairs of characters (or
character groups) that have to be considered open-close markers. For instance:
\begin{verbatim}
<Markers>
" "
( )
{ }
/* */
</Markers>
\end{verbatim}

The \verb#<SentenceEnd># section lists which characters are considered
as possible sentence endings. Each character is followed by a binary
value stating whether the character is an unambiguous sentence ending
or not. For instance, in the following example, ``?'' is an unambiguous
sentence marker, so a sentence split will be introduced
unconditionally after each ``?''.  The other two characters are not
unambiguous, so a sentence split will only be introduced if they are
followed by a capitalized word or a sentence start character.
\begin{verbatim}
<SentenceEnd>
. 0
? 1
! 0
</SentenceEnd>
\end{verbatim}

The \verb#<SentenceStart># section lists characters known to appear
only at sentence beggining. For instance, open question/exclamation
marks in Spanish:\\
\verb#<SentenceStart>#\\
{\tt ?` }\\
{\tt !` }\\
\verb#</SentenceStart>#

%..................................................
\section{Morphological Analyzer Module}
\label{sec-maco}

  The morphological analyzer is a meta-module which does not perform any processing of its own.

  It is just a convenience module to simplify the instantiation and
  call to the submodules described in the next sections (from \ref{file-numb} to \ref{file-prob}).

  At instantiation time, it receives a {\tt maco\_options} object, containing information
 about which submodules have to be created and which files have to be used to create them.

  A calling application may bypass this module and just call directly the submodules.

  The Morphological Analyzer API is:
\begin{verbatim}
class maco {
   public:
      /// Constructor. Receives a set of options.
      maco(const maco_options &); 

      /// analyze and enrich given sentences.
      void analyze(std::list<sentence> &);
};
\end{verbatim}

 The {\tt maco\_options} class has the following API:

\begin{verbatim}
class maco_options {

 public:
    /// Language analyzed
    std::string Lang;

    /// Submodules to activate
    bool AffixAnalysis,   MultiwordsDetection, 
         NumbersDetection, PunctuationDetection, 
         DatesDetection,   QuantitiesDetection, 
         DictionarySearch, ProbabilityAssignment;
    /// kind of NER wanted (NER_BASIC, NER_BIO, NER_NONE)
    int NERecognition;

    /// Names of data files to provide to each submodule.
    std::string LocutionsFile, QuantitiesFile, AffixFile, 
           ProbabilityFile, DictionaryFile, 
           NPdataFile, PunctuationFile;

    /// Extra parameters for Number Detection module
    std::string Decimal, Thousand;
    /// Extra parameters for Probability Assignment module
    double ProbabilityThreshold;

    /// constructor
    maco_options(const std::string &); 

    /// Option setting methods provided to ease perl interface generation. 
    /// Since option data members are public and can be accessed directly
    /// from C++, the following methods are not necessary, but may become
    /// convenient sometimes.
    /// The order of the parameters is the same they are defined above.
    void set_active_modules(bool,bool,bool,bool,bool,bool,bool,bool,int);
    void set_data_files(const std::string &,const std::string &,
                        const std::string &,const std::string &,
                        const std::string &,const std::string &,
                        const std::string &);
    void set_nummerical_points(const std::string &,const std::string &);
    void set_threshold(double);
\end{verbatim}

  To instantiate a Morphological Analyzer object, the calling application needs to 
  instantiate a {\tt maco\_options} object, initialize its fields with the desired values, 
  and use it to call the constructor of the {\tt maco} class.

  The created object will create the required sumbodules, and when asked to {\tt analyze}
 some sentences, it will just pass it down to each the submodule, and return the final result.

%..................................................
\section{Number Detection Module}
\label{file-numb}

  The number detection module is language dependent: It recognizes nummerical expression (e.g.: {\tt 1,220.54} or {two-hundred sixty-five}), and assigns them a normalized value as lemma.

  The module is basically a finite-state automata that recognizes valid nummerical expressions. Since the structure of the automata and the actions to compute the actual nummerical value are different for each lemma.

   For languages that do not have an implementation of a specific automata, a generic module is used to recognize number-like expressions that contain nummerical digits.

   For the reasons described so far, there is no options or configuration file to be provided to the class when it is instantiated. The API of the class is:
\begin{verbatim}  
class numbers {
   public:
      /// Constructor: receives the language code, and the decimal 
      /// and thousand point symbols
      numbers(const std::string &, const std::string &, const std::string &); 

      /// Detect number expressions in given sentence
      void annotate(sentence &);
};
\end{verbatim}  

   The parameters that the constructor expects are: 
\begin{itemize}
\item The language code: used to decide whether the generic recognizer or a language-specific module is used.
\item The decimal point symbol.
\item The thousand point sympol.
\end{itemize}
  The last two parameters are needed because in some latin languages, the comma is used as decimal point separator, and the dot as thousand mark, while in languages like English it is the other way round.
  These parameters make it possible to specify what character is to be expected at each of these positions. They will usually be comma and dot, but any character could be used.


%..................................................
\section{Punctuation Detection Module}
\label{file-punt}

 The punctuation detection module assigns Part-of-Speech tags to punctuation symbols. The API of the class is the following:
\begin{verbatim}  
class punts {
   public:
      /// Constructor: receives data file name
      punts(const std::string &); 
 
      /// Detect punctuation in given sentence
      void annotate(sentence &);
};
\end{verbatim}

 The constructor receives as parameter the name of a file containing the list of the PoS tags to be assigned to each punctuation symbol.

 Note that this module will be applied afer the tokenizer, so, it will only annotate symbols that have been separated at the tokenization step. For instance, if you include the three suspensive dots (\ldots) as a single punctuation symbol, it will have no effect unless the tokenizer has a rule that causes these substring to be tokenized in one piece.

\subsection{Punctuation Tags File}

 The format of the file listing the PoS for punctuation symbols is one
 punctuation symbol per line, each line with the format: {\tt punctuation-symbol tag}.\\
  E.g. \\ {\tt ! Fat}\\ {\tt , Fc} \\ {\tt
   : Fd} \\ {\tt ... Fs} \\

  One special line may be included for undefined punctuation symbols
  (any word with no alphanumeric character is considered a punctuation
  symbol). 

  This special line has the format: \verb#<Other> tag#.  E.g. \\
    \verb#<Other> Fz#

%..................................................
\section{Dates Detection Module}
\label{file-dates}

  The dates detection module, as the number detection module in section \ref{file-numb}, is a collection of language-specific finite-state automata, and for this reason needs no data file to be provided at instantiation time.

  For languages that do not have a specific automata, a default analyzer is used that detects simple date patterns (e.g. {\tt DD-MM-AAAA}, {\tt MM/DD/AAAA}, etc.)

  The API of the class is:
\begin{verbatim}        
class dates {             
   public:   
      /// Constructor: receives the language code
      dates(const std::string &); 

      /// Detect date/time expressions in given sentence
      void annotate(sentence &);
};  
\end{verbatim}        

   The only parameter expected by the constructor is the language of the text to analyze, in order to be able to apply the appropriate specific automata, or select the default one if none is available.


%..................................................
\section{Dictionary Search Module}
\label{file-dict}

The dictionary search module has two functions: Search the word forms in the dictionary to find out their lemmas and PoS tags, and apply affixation rules to find the same information in the cases in which the form is a derived form not included in the dictionary (e.g. the word {\tt quickly} may not be in the dictionary, but a suffixation rule may state that removing {\tt -ly} and searching for the obtained adjective is a valid way to form and adverb).

The decision of what is included in the dictionary and what is dealt with through affixation rules is left to the Linguistc Data developer.

The API for this module is the following:
\begin{verbatim}
class dictionary {
   public:
      /// Constructor
      dictionary(const std::string &, const std::string &, 
                bool, const std::string &);

      /// Get analysis for a given form, and add them
      /// to given analysis list
      void search_form(const std::string &, std::list<analysis> &);

      /// Analyze words in given sentence
      void annotate(sentence &);
}
\end{verbatim}

   The parameters of the constructor are:
\begin{itemize}
 \item The language of the processed text. This is required by the affixation submodule to properly handle graphical accents in latin languages.
 \item The dictionary file name. This must be a BerkeleyDB--indexed file. See below for details.
 \item A boolean stating whether affixation analysis has to be applied.
 \item The affixation rules file name (it may be an empty string if the boolean above is set to false) 
 \end{itemize}

\subsection{Form Dictionary File}
 
   The form dictionary is a BerkeleyDB--indexed file.

  It can be created with the {\tt indexdict} program provided with
  FreeLing, which is called with the command:
  \begin{verbatim}
   indexdict indexed-dict-name  <source-dict 
  \end{verbatim}
  See the (very simple) source code in {\tt src/main/utilities/indexdict.cc}
 if you're interested on how it is indexed.

  The {\tt source-dict} file must have the lemma--PoS list for a word
  form at each line.

  Each line has format: {\tt form lemma1 PoS1 lemma2 PoS2 ...}.  \\
  E.g.:\\
  \verb#casa casa NCFS000 casar VMIP3S0 casar VMM02S0# \\
  \verb#backs back NNS back VBZ#

  Lines corresponding to words that are contractions may have an
  alternative format if the contraction is to be splitted. The format
  is  {\tt form form1+form2+... PoS1+PoS2+...}. \\
  For instance:\\
  \verb#del de+el SPS+DA#
  
  This line expresses that whenever the form {\sl del} is found, it is
  replaced with two words:  {\sl de} and {\sl el}. Each of the new two
  word forms are searched in the dictionary, and assigned any tag
  matching their correspondig tag in the third field. So, {\sl de}
  will be assigned all tags starting with {\tt SPS} that this
  entry may have in the dictionary, and {\sl el} will get any
  tag starting with {\tt DA}.
  
  Note that a contraction cannot be splitted in two different ways
  corresponding to different forms (e.g. {\tt he's = he+is | he+has}),
  so only a combination of forms and a combination of tags may appear
  in the dictionary.

   Nevertheless, a set of tags may be specified for a given form, e.g.:\\
  \verb#he'd he+'d PRP+VB/MD#

   This will produce two words: {\sl he} with {\tt PRP} analysis, and
   {\sl 'd} with its analysis matching any of the two given tags
   (i.e. {\tt have\_VBZ} and {\tt would\_MD}).  Note that this will
   work only if the form {\sl 'd} is found in the dictionary with
   those possible analysis.

   If all tags for one of the new forms are to be used, a wildcard may
   be written as a tag. E.g.:\\
  \verb#pal para+el SPS+*#

   This will replace {\sl pal} with two words, {\sl para} with only its
   {\tt SPS} analysis, plus {\sl el} with all its possible tags.  


\subsection{Affixation Rules File}
\label{file-suf}

 The submodule of the dictionary handler that deals with affixes requires a set of affixation rules.

 The file consists of two (optional) sections: \verb#<Suffixes># and \verb#<Prefixes>#. The first one contains suffixation rules, and the second, prefixation rules. They may appear in any order.

 Both kinds of rules have the same format, and only differ in whether the affix is checked at the beggining or at the end of the word.

Each rule has to be written in a different line, and has 10 fields:
\begin{enumerate}
 \item Affix to erase form word form (e.g: crucecita - cecita = cru)
 \item Affix (* for emtpy string) to add to the resulting root to rebuild the lemma that must be searched in dictionary  (e.g. cru + z = cruz)
 \item Condition on the parole tag of found dictionary entry
 (e.g. cruz is NCFS). The condition is a perl RegExp
 \item Parole tag for suffixed word (* = keep tag in dictionary entry)
 \item Check lemma adding accents
 \item Enclitic suffix (special accent behaviour in Spanish)
 \item Prevent later modules (e.g. probabilities) from assigning additional tags to the word
 \item Lemma to assign: Any combination of: {\tt F}, {\tt R}, {\tt L}, {\tt A}, or a string literal separated with a {\tt +} sign. For instance: {\tt R+A}, {\tt A+L}, {\tt R+mente}, etc.

  {\tt F} stands for the original form (before affix removal, e.g. {\em crucecitas}), {\tt R} stands for root found in dictionary (after affix removal and root reconstruction,  e.g. {\em cruces}), {\tt L} stands for lemma in matching dictionary entry (e.g. {\em cruz}), {\tt A}stands for the affix that the rule removed

 \item Try the affix always, not only for unknown words.

 \item Retokenization info, explained below (``\verb#-#'' for none)
\end{enumerate}

 \noindent E.g., prefix rules:\\
 \verb#anti    *     ^NC    AQ0CN0   0  0  1  A+L 0  -#

  This prefix rule states that {\tt anti} should be removed from the beggining of the word, nothing ({\tt *}) should be added, and the resulting root should be found in the dictionary with a NC PoS tag. If that is satisfied, the word would receive the {\tt AQ0CN0} tag and the affix ({\tt anti}) plus the lemma as the lemma of the prefixed word.
  For instance, the word {\tt antimisiles} would match this rule: {\tt misiles} would be found in the dictionary with lema {\tt misil} and PoS {\tt NCMP000}. Then, the word will be assigned the lemma {\tt antimisil} ({\tt A+L = anti+misil}) and the tag AQ0CN0.

 \noindent E.g., sufix rules:\\
  \verb#cecita  z|za  ^NCFS  NCFS00A  0  0  1  L   0  -#\\
  \verb#les     *     ^V      *       0  1  0  L   1  $$+les:$$+PP#

  The first suffix rule above ({\tt cecita}) states a suffix rule that will be
  applied to unknown words, to see whether a valid feminine singular
  noun is obtained when substituting the suffix {\tt cecita} with {\tt
  z} ot {\tt za}. This is the case of {\tt crucecita} (diminutive of
  {\tt cruz}). If such a base form is found, the original word is
  analyzed as diminutive suffixed form. No retokenization is performed.

  The second rule ({\tt les}) applies to all words and tries to check
  whether a valid verb form is obtained when removing the suffix {\tt
  les}. This is the case of words such as {\tt viles} (which may mean 
  {\sl I saw them}, but also is the plural of the adjective {\tt
  vil}). In this case, the retokenization info states that if
  eventually the verb tag is selected for this word, it may be
  retokenized in two words: The base verb form (referred to as {\tt
  \$\$}, {\tt vi} in the example) plus the word {\tt les}. The tags
  for these new words are expressed after the colon: The base form
  must keep its PoS tag (this is what the second {\tt \$\$} means)
  and the second word may take any tag starting with PP it may have in
  the dictionary.

  So, for word {\tt viles} would obtain its adjective analysis from
  the dictionary, plus its verb + clitic pronoun from the suffix
  rule:\\
  \verb#viles vil AQ0CP0 ver VMIS1S0#

  The second analysis will carry the retokenization information, so if
  eventually the PoS tagger selects the {\tt VMI} analysis (and the
  TaggerRetokenize option is set), the word will be retokenized into:
\begin{verbatim}
   vi ver VMIS1S0
   les ellos PP3CPD00
\end{verbatim}


%..................................................
\section{Multiword Recognition Module}
\label{file-mw}

  This module aggregates input tokens in a single word object if they are found in a given list of multiwords.

  The API for this class is:
\begin{verbatim}
class automat {
 public:
      /// Constructor
      automat();

      /// Detect patterns in given sentence
      void annotate(sentence &);
};

class locutions: public automat {
   public:
      /// Constructor, receives the name of the file
      ///  containing the multiwords to recognize.
      locutions(const std::string &);
};
\end{verbatim}

  Class {\tt automat} implements a generic FSA. The {\tt locutions} class is a derived class
which implements a FSA to recognize the word patters listed in the file given to the constructor.

\subsection{Multiword Definition File}

The file contains a list of multiwords to be recognized. The format of
the file is one multiword per line. Each line has three fields: the
multiword form, the multiword lemma, and the multiword PoS tag.

The multiword form may admit lemmas in angle brackets, meaning that
any form with that lemma will be considered a valid component for the
multiword.

 For instance:
\begin{verbatim}
a_buenas_horas a_buenas_horas RG
a_causa_de a_causa_de SPS00
<accidente>_de_trabajo accidente_de_trabajo $1:NC
\end{verbatim}
 
 The tag may be specified directly, or as a reference to the tag of
some of the multiword components. In the previous example, the last
multiword specification will build a multiword with any of the forms {\tt
 accidente de trabajo} or {\tt accidentes de trabajo}. The tag of the
multiword will be that of its first form ({\tt \$1}) which starts with
{\tt NC}.  This will assign the right singular/plural tag to the
multiword, depending on whether the form was ``accidente'' or ``accidentes''.


%..................................................
\section{Named Entity Recognition Module}
\label{file-ner}

  There are two different modules able to perform NE recognizer. The application should decide which method is to be used, and instantiate the right class.

  The first NER module is the {\tt np} class, which is a just a FSA that basically detects sequences of capitalized words, taking into account some functional words (e.g. {\em Bank of England}) and captialization at sentence begginings.
  
  The second module, named {\tt bioner}, is based on machine learning algorithms in Omlet\&Fries libraries, and has to be trained from a tagged corpus.

  The {\tt np} module is simple and fast, and easy to adapt for use in new languages, provided capitalization is the basic clue for NE detection. The estimated performance of this module is about 85\% correctly recognized named entities. Its API is the following:
\begin{verbatim}
class np: public ner, public automat {
  public:
    /// Constructor, receives a configuration file.
    np(const std::string &); 

    /// ("annotate" is inherited from "automat")
    void annotate(sentence &);
};
\end{verbatim}

  The {\tt bioner} module has a higher precision (over 90\%), but is much slower, and adaptation to new languages requires a training corpus, and some feature engineering.
\begin{verbatim}
class bioner: public ner {
  public:
    /// Constructor, receives the name of the configuration file.
    bioner ( const std::string & );

    /// Recognize NEs in given sentence
    void annotate ( sentence & );
};
\end{verbatim}


\subsection{Basic NER Options File (module {\tt np}) }

  The file that controls the behaviour of the simple NE recognizer
  consists of the following sections:

\begin{itemize}
  \item Section \verb#<FunctionWords># lists the function words that can be
  embeeded inside a proper noun (e.g. preposisions and articles such
  as those in ``Banco de España'' or ``Foundation for the Eradication
  of Poverty''). For instance:
\begin{verbatim}
<FunctionWords>
el
la
los
las
de
del
para
</FunctionWords>
\end{verbatim}

 \item Section \verb#<SpecialPunct># lists the PoS tags (according to
  punctuation tags definition file, section \ref{file-punt}) after
  which a capitalized word {\sl may} be indicating just a sentence or clause
  beggining and not necessarily a named entity. Typical cases are
  colon, open parenthesis, dot, hyphen..
\begin{verbatim}
<SpecialPunct>
Fpa
Fp
Fd
Fg
</SpecialPunct>
\end{verbatim}

  \item Section \verb#<NE_Tag># contains only one line with the PoS tag that
  will be assigned to the recognized entities. If the NE classifier is
  going to be used later, it will have to be informed of this tag at
  creation time.
\begin{verbatim}
<NE_Tag>
NP00000
</NE_Tag>
\end{verbatim}

  \item Section \verb#<Ignore># contains a list of forms (lowercased)
    or PoS tags (uppercased) that are not to be considered a named
    entity even when they appear capitalized in the middle of a
    sentence.  For instance, the word {\em Spanish} in the sentence
    {\em He started studying Spanish two years ago} is not a named
    entity. If the words in the list appear with other capitalized
    words, they are considered to form a named entity (e.g. {\em An
      announcement of the Spanish Bank of Commerce was issued
      yesterday}). The same distinction applies to the word {\em I} in
    the sentences {\em whatever you say, I don't believe}, and {\em
      That was the death of Henry I}.

     Each word or tag is followed by a $0$ or $1$ indicating whether
     the {\sl ignore} condition is strict ($0$: non-strict, $1$:
     strict).  The entries marked as non-strict will have the
     behaviour described above.  The entries marked as strict will
     {\sl never} be considered named entities or NE parts.

     For instance, the following \verb#<Ignore># section states that
     the word ``I'' is not to be a proper noun ({\em whatever you say,
       I don't believe}) unless some of its neighbour words are ({\em
       That was the death of Henry I}). It also states that any word
     with the {\tt RB} tag, and any of the listed language names must
     {\sl never} be considered as possible NEs.
\begin{verbatim}
<Ignore>
i  0
RB 1
english 1
dutch 1
spanish 1
</Ignore>
\end{verbatim}


  \item Section \verb#<Names># contains a list of lemmas that may be names, even if they conflict with some of the heuristic criteria used by the NE recognizer. This is useful when they appear capitalized at sentence beggining. For instance, the basque name {\em Miren} (Mary) or the nickname {\em Pelé} may appear at the beggining of a Spanish sentence. Since both of them are verbal forms in Spanish, they would not be considered candidates to form named entities. 
 
   Including the form in the \verb#<Names># section, causes the NE choice to be added to the possible tags of the form, giving the tagger the chance to decide whether it is actually a verb or a proper noun.
\begin{verbatim}
<Names>
miren
pelé
zapatero
china
</Names>
\end{verbatim}


  \item Sections \verb#<RE_NounAdj># \verb#<RE_Closed># and \verb#<RE_DateNumPunct># allow to modify the default regular expressions for PAROLE Part-of-Speech tags. This regular expressions are used by the NER to determine whether a sentence-beginning word has some tag that is Noun or Adj, or any tag that is a closed category, or one of date/punctuation/number. The default is to check against PAROLE tags, thus, the recognizer will fail to identifiy these categories if your dictionary uses another tagset, unless you specify the right patterns to look for.

  For instance, if our dictionary uses Penn-Treebank-like tags, we should define:
\begin{verbatim}
<RE_NounAdj>
^(NN$|NNS|JJ)
</RE_NounAdj>
<RE_Closed>
^(D|IN|C)
</RE_Closed>
\end{verbatim}
  
  \item Section \verb#<TitleLimit># contains only one line with an integer
  value stating the length beyond which a sentence written {\sl
  entirely} in uppercase will be considered a title and not a proper
  noun. Example:
\begin{verbatim}
<TitleLimit>
3
</TitleLimit>
\end{verbatim}

  If \verb#TitleLimit=0# (the default) title detection is
  deactivated (i.e, all-uppercase sentences are always marked as
  named entities).

  The idea of this heuristic is that newspaper titles are usually
  written in uppercase, and tend to have at least two or three
  words, while named entities written in this way tend to be acronyms
  (e.g. IBM, DARPA, ...) and usually have at most one or two words.

  For instance, if \verb#TitleLimit=3# the sentence 
  {\tt FREELING ENTERS NASDAC UNDER CLOSE OB\-SER\-VA\-TION OF MARKET ANALYSTS}
  will not be recognized as a named entity, and will have its words analyzed
  independently. On the other hand, the sentence {\tt IBM INC.}, having less than
  3 words, will be considered a proper noun.

  Obviously this heuristic is not 100\% accurate, but in some cases
  (e.g. if you are analyzing newspapers) it may be preferrable to the
  default behaviour (which is not 100\% accurate, either).
  
    \item Section \verb#<SplitMultiwords># contains only one line with either \verb#yes# or \verb#no#. If \verb#SplitMultiwords# is activated Named Entities still will be recognized but they will not be treated as a unit with only one Part-of-Speech tag for the whole compound. Each word gets its own Part-of-Speech tag instead.\\
    Capitalized words get the Part-of-Speech tag as specified in \verb#NE_Tag#, The Part-of-Speech tags of non-capitalized words inside a Named Entity (typically, prepositions and articles) will be left untouched.
\begin{verbatim}
<SplitMultiwords>
no
</SplitMultiwords>
\end{verbatim}
\end{itemize}

%%%%%

\subsection{{\em BIO} NER Options File (module {\tt bioner})}

  The machine-learning based NER module requieres a different configuration file. It consists of the following sections:

\begin{itemize}
  \item Section \verb#<RGF># contains one line with the path to the RGF file of the model. This file is the definition of the features that will be taken into account for NER.  These features are processed by {\tt libfries}. 
\begin{verbatim}
<RGF>
ner.rgf
</RGF>
\end{verbatim}

  \item Section \verb#<AdaBoostModel># contains one line with the path to the model file learnt with AdaBoost. These models are learnt and used by {\tt libomlet}.
\begin{verbatim}
<AdaBoostModel>
ner.abm
</AdaBoostModel>
\end{verbatim}

\item Section \verb#<Lexicon># contains one line with the path to the lexicon file of the learnt model. The lexicon is used to translate string-encoded features generated by {\tt libfries} to integer-encoded features needed by {\tt libomlet}. The lexicon file is generated by {\tt libfries} at training time.
\begin{verbatim}
<Lexicon>
ner.lex
</Lexicon>
\end{verbatim}

\item Section \verb#<Classes># contains only one line with the classes of the model and its translation to B, I, O tag.
\begin{verbatim}
<Classes>
0 B 1 I 2 O
</Classes>
\end{verbatim}

\item Section \verb#<InitialProb># Contains the probabilities of seeing each class at the begining of a sentence. These probabilities are necessary for the Viterbi algorithm used to annotate NEs in a sentence.
\begin{verbatim}
<InitialProb>
B 0.200072
I 0.0
O 0.799928
</InitialProb>
\end{verbatim}

\item Section \verb#<TransitionProb># Contains the transition probabilities for each class to each other class, used by the Viterbi algorithm.
\begin{verbatim}
<TransitionProb>
B B 0.00829346
B I 0.395481
B O 0.596225
I B 0.0053865
I I 0.479818
I O 0.514795
O B 0.0758838
O I 0.0
O O 0.924116
</TransitionProb>
\end{verbatim}


\item Section \verb#<TitleLimit># contains only one line with an integer
  value stating the length beyond which a sentence written {\sl
  entirely} in uppercase will be considered a title and not a proper
  noun. Example:
\begin{verbatim}
<TitleLimit>
3
</TitleLimit>
\end{verbatim}

  If \verb#TitleLimit=0# (the default) title detection is
  deactivated (i.e, all-uppercase sentences are always marked as
  named entities).

  The idea of this heuristic is that newspaper titles are usually
  written in uppercase, and tend to have at least two or three
  words, while named entities written in this way tend to be acronyms
  (e.g. IBM, DARPA, ...) and usually have at most one or two words.

  For instance, if \verb#TitleLimit=3# the sentence 
  {\tt FREELING ENTERS NASDAC UNDER CLOSE OB\-SER\-VA\-TION OF MARKET ANALYSTS}
  will not be recognized as a named entity, and will have its words analyzed
  independently. On the other hand, the sentence {\tt IBM INC.}, having less than
  3 words, will be considered a proper noun.

  Obviously this heuristic is not 100\% accurate, but in some cases
  (e.g. if you are analyzing newspapers) it may be preferrable to the
  default behaviour (which is not 100\% accurate, either).
  
  \item Section \verb#<SplitMultiwords># contains only one line with either \verb#yes# or \verb#no#. If \verb#SplitMultiwords# is activated Named Entities still will be recognized but they will not be treated as a unit with only one Part-of-Speech tag for the whole compound. Each word gets its own Part-of-Speech tag instead.\\
    Capitalized words get the Part-of-Speech tag as specified in \verb#NE_Tag#, The Part-of-Speech tags of non-capitalized words inside a Named Entity (typically, prepositions and articles) will be left untouched.
\begin{verbatim}
<SplitMultiwords>
no
</SplitMultiwords>
\end{verbatim}
\end{itemize}


%..................................................
\section{Quantity Recognition Module}
\label{file-quant}

  The {\tt quantities} class is a FSA that recognizes ratios, percentages, and physical or currency magnitudes (e.g. {\em twenty per cent}, {\em 20\%}, {\em one out of five}, {\em 1/5},{\em one hundred miles per hour}, etc.

  This module depends on the numbers detection module (section \ref{file-numb}). If numbers are not previously detected and annotated in the sentence, quantities will not be recognized.

   This module, similarly to number recognition, is language dependent: That is, a FSA has to be programmed to match the patterns of ratio expressions in that language.

   Currency and physical magnitudes can be recognized in any language, given the appropriate data file.
\begin{verbatim}
class quantities {
   public:
      /// Constructor: receives the language code, and the data file.
      quantities(const std::string &, const std::string &); 
      /// Detect magnitude expression in given sentence
      void annotate(sentence &);
};
\end{verbatim}


\subsection {Quantity Recognition Data File}
This file contains the data necessary to perform currency amount and
physical magnitude recognition.  
It consists of three sections:  \verb#<Currency>#, \verb#<Measure>#,
and \verb#</MeasureNames>#.

Section \verb#<Currency># contains a single line indicating which is
the code, among those used in section \verb#<Measure>#, that stands for
 'currency amount'.  This is used to assign to currency ammounts a different
 PoS tag than physical magnitudes.
E.g.:
\begin{verbatim}
<Currency>
CUR
</Currency>
\end{verbatim}

Section \verb#<Measure># indicates the type of measure corresponding
to each possible unit. Each line contains two fields: the measure code
and the unit code. The codes may be anything, at user's choice, and
will be used to build the lemma of the recognized quantity multiword.

E.g., the following section states that {\tt USD} and {\tt FRF} are of
type {\tt CUR} (currency), {\tt mm} is of type {\tt LN} (length), and 
{\tt ft/s} is of type {\tt SP} (speed):
\begin{verbatim}
<Measure>
CUR USD
CUR FRF
LN mm
SP ft/s
</Measure>
\end{verbatim}

Finally, section \verb#<MeasureNames># describes which multiwords have
to be interpreted as a measure, and which unit they represent. The
unit must appear in section \verb#<Measure># with its associated code.
Each line has the format:
\begin{verbatim}
multiword_description code tag
\end{verbatim}
where {\tt multiword\_description} is a multiword pattern as in
multiwords file described in section~\ref{file-mw}, {\tt code} is the
type of magnitude the unit describes (currency, speed, etc.), and {\tt
tag} is a constraint on the lemmatized components of the multiword,
following the same conventions than in multiwords file
(section~\ref{file-mw}).

E.g., 
\begin{verbatim}
<MeasureNames>
french_<franc> FRF $2:N
<franc> FRF $1:N
<dollar> USD $1:N
american_<dollar> USD $2:N
us_<dollar> USD $2:N
<milimeter> mm $1:N
<foot>_per_second ft/s $1:N
<foot>_Fh_second ft/s $1:N
<foot>_Fh_s ft/s $1:N
<foot>_second ft/s $1:N
</MeasureNames>
\end{verbatim}

This section will recognize strings such as the following:
\begin{verbatim}
 234_french_francs CUR_FRF:234 Zm
 one_dollar CUR_USD:1 Zm
 two_hundred_fifty_feet_per_second SP_ft/s:250 Zu
\end{verbatim}

 Quantity multiwords will be recognized only when following a number,
 that is, in the sentence {\em There were many french francs}, the
 multiword won't be recognized since it is not assigning units to a
 determined quantity.

 It is important to note that the lemmatized multiword expressions
 (the ones that containt angle brackets) will only be recognized if
 the lemma is present in the dictionary with its corresponding
 inflected forms.


%..................................................
\section{Probability Assignment and Unkown Word Guesser Module}
\label{file-prob}

 This class ends the morphological analysis subchain, and has two
 functions: first, it assigns an {\em a priori} probability to each
 analysis of each word. These probablities will be needed for the PoS
 tagger later. Second, if a word has no analysis (none of the previously
 applied modules succeeded to analyze it), this module tries to guess
 which are its possible PoS tags, based on the word ending.
\begin{verbatim}
class probabilities {
   public:
      /// Constructor: receives the language code, the name of the file
      // containing probabilities, and a threshold.
      probabilities(const std::string &, const std::string &, double);

      /// Assign probabilities to all analysis of each word in sentence
      void annotate(sentence &);
      /// Assign probabilities to all analysis of given word
      void annotate_word(word &);
};
\end{verbatim}

  The constructor receives:
\begin{itemize}
\item The language code: It is used only to decide whether an EAGLES tagset is being used, and to appropriately shorten the tags if that's the case.
\item The probabilities file name: The file that contains all needed statistical information. This file can be generated from a tagged training corpus using the scripts in {\tt src/utilities}. Its format is described below.
\item A threshold: This is used for unknown words, when the probability of each possible tag has been estimated by the guesser according to word endings, tags with a value lower than this threshold are discarded.
\end{itemize}

\subsection{Lexical Probabilities File}

This file can be generated from a tagged corpus using the {script {\tt
 src/utilitities/TRAIN} provided in FreeLing
package. See comments in the script file to find out which format
 the corpus is expected to have.

The probabilities file has six sections:
\verb#<UnknownTags>#, \verb#<Theeta>#, \verb#<Suffixes>#, \verb#<SingleTagFreq>#, \verb#<ClassTagFreq>#, \verb#<FormTagFreq>#. Each section is closed by it corresponding tag \verb#</UnknownTags>#, \verb#</Theeta>#, \verb#</Suffixes>#,  \verb#</SingleTagFreq>#, \verb#</ClassTagFreq>#, \verb#</FormTagFreq>#.
 \begin{itemize} 

  \item Section \verb#<FormTagFreq>#.  Probability data of some high frequency forms.

   If the word is found in this list, lexical probabilities are computed using data in \verb#<FormTagFreq># section.
 
   The list consists of one form per line, each line with format: \\
   {\tt form ambiguity-class, tag1 \#observ1 tag2 \#observ2 ...}
   
   E.g. {\tt japonesas AQ-NC AQ 1 NC 0}

   Form probabilities are smoothed to avoid zero-probabilities. 

   \item Section \verb#<ClassTagFreq>#. Probability data of ambiguity  classes.

   If the word is not found in the \verb#<FormTagFreq>#, frequencies for its ambiguity class are used.

   The list consists of class per line, each line with format:\\
   {\tt class tag1 \#observ1 tag2 \#observ2 ...}

   E.g. {\tt AQ-NC AQ 2361 NC 2077}

   Class probabilities are smoothed to avoid zero-probabilities.  

   \item Section \verb#<SingleTagFreq>#. Unigram probabilities. 

   If the ambiguity class is not found in the \verb#<ClassTagFreq>#, individual
   frequencies for its possible tags are used.

   One tag per line, each line with format: {\tt tag \#observ}

    E.g. {\tt AQ 7462}

   Tag probabilities are smoothed to avoid zero-probabilities.


  \item Section \verb#<Theeta>#.    Value for parameter {\it theeta} used  in smoothing of tag probabilities based on word suffixes.

   If the word is not found in dictionary (and so the list of its
   possible tags is unknown), the distribution is computed using the
   data in the \verb#<Theeta>#, \verb#<Suffixes>#, and \verb#<UnknownTags># sections.

   The  section has exactly one line, with one real number.

   E.g. \\
   {\tt \verb#<Theeta>#\\
   0.00834\\
   \verb#</Theeta>#}

  \item Section \verb#<Suffixes>#.  List of suffixes obtained from a
    train corpus, with information about which tags were assigned to
    the word with that suffix. 

   The list has one suffix per line, each line with format: {\tt suffix \#observ tag1 \#observ1 tag2 \#observ2 ...}

   E.g. \\
   {\tt orada 133 AQ0FSP 17 VMP00SF 8 NCFS000 108}

  \item Section \verb#<UnknownTags>#. List of open-category tags to
  consider as possible candidates for any unknown word. 

   One tag per line, each line with format: {\tt tag \#observ}.  The tag is the complete Parole label. The count is the number of occurrences in a training corpus.

   E.g. {\tt NCMS000 33438}
\end{itemize}


%..................................................
\section{Sense Labelling Module}
\label{mod-sense}

  This module searches the lemma of each analysis in a sense dictionary,
  and enriches the analysis with the list of senses found there.

  Note that this is not disambiguation, all senses for the lemma are returned. 

  The module receives a file containing the sense dictionary. FreeLing
  provides WordNet-based \cite{fellbaum98,vossen98a} dictionaries, but
  the results of this module can be changed to any other sense
  catalogue simply providing a different sense dictionary
  file.

\begin{verbatim}
class senses {
   public:
      /// Constructor: receives the name of the dictionary file and a boolean.
      senses(const std::string &, bool); 
 
      /// sense annotate selected analysis for each word in given sentences
      void analyze(std::list<sentence> &);
};
\end{verbatim}

   The constructor of this class receives:
\begin{itemize}
\item The name of the sense dictionary file. This file is a Berkely DB indexed file, containing sense codes for each lemma-PoS. Its contents can be build as described in section \ref{file-sense}.
\item A boolean stating whether the analysis with more than one sense must be duplicated.

  For instance, the word {\em crane} has the follwing analysis:
 \begin{verbatim}
    crane 
       crane NN  0.833
       crane VB  0.083
       crane VBP 0.083
 \end{verbatim}

  If the list of senses is simply added to each of them (that is, the
  {\tt duplicate} boolean is set to {\tt false}), you will get:
 \begin{verbatim}
    crane 
       crane NN  0.833  02516101:01524724
       crane VB  0.083  00019686
       crane VBP 0.083  00019686
 \end{verbatim}

   But if you set the boolean to {\t true}, the {\tt NN} analysis will be duplicated for each of its possible senses:
 \begin{verbatim}
    crane 
       crane NN  0.416  02516101
       crane NN  0.416  01524724
       crane VB  0.083  00019686
       crane VBP 0.083  00019686
 \end{verbatim}
\end{itemize}

%..................................................
\section{Word Sense Disambiguation Module}
\label{mod-ukb}

  This module performs word-sense-disambiguation on content words in
  given sentences. 
  This module is to be used if word sense disambiguation (WSD) is
  desired.  If no disambiguation (or basic most-frequent-sense
  disambiguation) is needed, the senses module described in section
  \ref{mod-sense} is a lighter and faster option.

  The module is just a wrapper for UKB algorithm
  \cite{agirre09}, which is integrated in FreeLing and distributed as-is
   under its original GPL license.

  UKB algorithm relies on a semantic relation network (in this case,
  WN and XWN) to disambiguate the most likely senses for words in a
  text using PageRank algorithm. See \cite{agirre09} for details on the
  algorithm.

 The module enriches each analysis of each word (for the selected PoS)
 with a ranked list of senses. The PageRank value is also provided as
 a result.

  The API of the class is the following:
\begin{verbatim}
class disambiguator {
   public:
      /// Constructor. Receives a relation file for UKB, a sense dictionary,
      /// and two UKB parameters: epsilon and max iteration number.
      disambiguator(const std::string &, const std::string &, double, int);

      /// word sense disambiguation for each word in given sentences
      void analyze(std::list<sentence> &);
};
\end{verbatim}

  The constructor receives:
\begin{itemize}
  \item The file name of the semantic relationship graph to load. This
    is a binary file containing pairs of related senses (WN synsets in
    this case). Relations are not labelled nor directed.

    This file is created from the plain file
    \verb#data/common/wn16-ukb.src# by FreeLing installation scripts.
    You can re-create it (or create a new one using a different
    relation set) with the following command:
\begin{verbatim}
   compile_ukb $FLSHARE/common/wn16-ukb.src $FLSHARE/common/wn16-ukb.bin
\end{verbatim}
    (\verb#$FLSHARE# refers to the \verb#share/FreeLing# directory in your
FreeLing installation, which defaults to \verb#/usr/local/share/FreeLing#)

    \item The second parameter is a sense dictionary, with the
      possible senses for each word. Since this is the same
      information contained in the sense file for the Senses module
      described in section \ref{mod-sense}, it is generated by
      FreeLing installation scripts simply converting the default
      sense file (see sections \ref{mod-sense} and \ref{file-sense})
      to the appropriate format.

      You can re-create this file (or use any other sense dictionary
    with the right format) with the command (paths may differ):
\begin{verbatim}
  convertdict <$FLSHARE/es/senses16.src >$FLSHARE/es/senses16.ukb
\end{verbatim}
    (\verb#$FLSHARE# refers to the \verb#share/FreeLing# directory in your
    FreeLing installation, which defaults to \verb#/usr/local/share/FreeLing#.
    Obviously, if you want to convert the file for a language different 
    than Spanish, you have to use the right path).

  \item The last two parameters are UKB parameters: The an {\sl
    epsilon} float value that controls the precision with with the end
    of PageRank iterations is decided, and a {\sl MaxIterations}
    integer, that controls the maximum number of PageRank iterations,
    even is no convergence is reached.
 \end{itemize}

%..................................................
\section{Part-of-Speech Tagger Module}
\label{sec-pos}


  There are two different modules able to perform PoS tagging. The application should decide which method is to be used, and instantiate the right class.

  The first PoS tagger is the {\tt hmm\_tagger} class, which is a classical trigam Markovian tagger, following \cite{brants00}.

  The second module, named {\tt relax\_tagger}, is a hybrid system capable to integrate statistical and hand-coded knowledge, following \cite{padro98a}.

  The {\tt hmm\_tagger} module is somewhat faster than {\tt relax\_tagger} but slightly less accurate (specially if you use manual constraints with the later). Its API is the following:
\begin{verbatim}
class hmm_tagger: public POS_tagger {
   public:
       /// Constructor
       hmm_tagger(const std::string &, const std::string &, bool, unsigned int);

       /// disambiguate given sentences 
       void analyze(std::list<sentence> &);
};
\end{verbatim}

  The {\tt hmm\_tagger} constructor receives the following parameters:
\begin{itemize}
\item The language code: Used to determine if the language uses an EAGLES tagset, and to properly shorten the PoS tags in that case.
\item The HMM file, which containts the model parameters. The format
  of the file is described below. This file can be generated from a
  tagged corpus using the script {\tt src/utilitities/TRAIN} provided
    in FreeLing package. See comments in the script file to find out
    which format the corpus is expected to have.
\item A boolean stating whether words that carry retokenization information (e.g. set by the dictionary or affix handling modules) must be retokenized (that is, splitted in two or more words) after the tagging.
\item An integer stating whether and when the tagger must select only one analysis in case of ambiguity. Possbile values are: {\tt FORCE\_NONE (or 0)}: no selection forced, words ambiguous after the tagger, remain ambiguous.  {\tt FORCE\_TAGGER (or 1)}: force selection immediately after tagging, and before retokenization. {\tt FORCE\_RETOK (or 2)}: force selection after retokenization.
\end{itemize}


  The {\tt relax\_tagger} module has a higher precision, and can be tuned with hand written constraint, but is about 1.5 or 2 times slower than {\tt hmm\_tagger}.
\begin{verbatim}
class relax_tagger : public POS_tagger {
   public:
       /// Constructor, given the constraint file and config parameters
       relax_tagger(const std::string &, int, double, double, bool, unsigned int);

       /// disambiguate sentences
       void analyze(std::list<sentence> &);
};
\end{verbatim}

  The {\tt relax\_tagger} constructor receives the following parameters:
\begin{itemize}
\item The constraint file. The format
  of the file is described below. This file can be generated from a
  tagged corpus using the script {\tt src/utilitities/TRAIN} provided
    in FreeLing package. See comments in the script file to find out
    which format the corpus is expected to have.
\item An integer stating the maximum number of iterations to wait for
   convergence before stopping the disambiguation algorithm.
\item A real number representing the scale factor of the constraint weights.
\item A real number representing the threshold under which any changes
  will be considered too small. Used to detect convergence.
\item A boolean stating whether words that carry retokenization information (e.g. set by the dictionary or affix handling modules) must be retokenized (that is, splitted in two or more words) after the tagging.
\item An integer stating whether and when the tagger must select only one analysis in case of ambiguity. Possbile values are: {\tt FORCE\_NONE (or 0)}: no selection forced, words ambiguous after the tagger, remain ambiguous.  {\tt FORCE\_TAGGER (or 1)}: force selection immediately after tagging, and before retokenization. {\tt FORCE\_RETOK (or 2)}: force selection after retokenization.
\end{itemize}

  The iteration number, scale factor, and threshold parameters are very specific of the relaxation labelling algorithm. Refer to \cite{padro98a} for details.


\subsection{HMM-Tagger Parameter File}
\label{file-hmm}

   This file contains the statistical data for the Hidden Markov
   Model, plus some additional data to smooth the missing values.
   Initial probabilities, transition probabilities, lexical
   probabilities, etc.

   The file can be generated from a
    tagged corpus using the script {\tt src/utilitities/TRAIN} provided
    in FreeLing package. See comments in the script file to find out
    which format the corpus is expected to have.

  The file has six
  sections: \verb#<Tag>#, \verb#<Bigram>#, \verb#<Trigram>#, \verb#<Initial>#,
  \verb#<Word>#, and \verb#<Smoothing>#. Each section is closed by it
  corresponding tag \verb#</Tag>#, \verb#</Bigram>#, \verb#</Trigram>#, etc.

  The tag (unigram), bigram, and trigram probabilities are used in
  Linear Interpolation smoothing by the tagger.
 
  \begin{itemize} 
\itemsep 0.2cm
    \item Section \verb#<Tag>#. List of unigram tag probabilities
  (estimated via your preferred method). 
  Each line is a tag probability {\tt P(t)} with format \\
  {\tt Tag Probability}

   Lines for zero tag (for initial states) and
  for {\tt x} (unobserved tags) must be included.

  E.g.\\
    {\tt 0  0.03747}\\
    {\tt AQ 0.00227}\\
    {\tt NC 0.18894}\\
    {\tt x  1.07312e-06}

    \item Section \verb#<Bigram>#. List of bigram
  transition probabilities (estimated via your preferred method), 
   Each line is a transition probability, with the format:\\
 {\tt Tag1.Tag2 Probability}

  Tag zero indicates sentence-beggining.

    E.g. the following line indicates the transition probability between a
    sentence start and the tag of the first word being {\tt AQ}.\\
    {\tt 0.AQ 0.01403}

    E.g. the following line indicates the transition probability between two
    consecutive tags.\\
    {\tt AQ.NC 0.16963}

    \item Section \verb#<Trigram>#. List of trigram
  transition probabilities (estimated via your preferred method), 

   Each line is a transition probability, with the format:\\
 {\tt Tag1.Tag2.Tag3 Probability}.
   Tag zero indicates sentence-beggining.

    E.g. the following line indicates the transition probability that
    after a {\tt 0.AQ} sequence, the next word has {\tt NC} tag.\\
    {\tt 0.AQ.NC 0.204081}

    E.g. the following line indicates the probability of a tag {\tt
    SP} appearing after two words tagged {\tt DA} and {\tt NC}.\\
    {\tt DA.NC.SP 0.33312}

    \item Section \verb#<Initial>#. List of initial state probabilities
  (estimated via your preferred method), i.e. the ``pi'' parameters of
  the HMM. 

   Each line is an initial probability, with the format {\tt
   InitialState LogProbability}.

   Each state is a PoS-bigram code with the
   form {\tt 0.tag}. Probabilities are given in logarithmic form to avoid
   underflows.

    E.g. the following line indicates the probability that the
    sequence starts with a determiner.\\
    {\tt 0.DA -1.744857}

    E.g. the following line indicates the probability that the
    sequence starts with an unknown tag.\\
    {\tt 0.x -10.462703}

    \item Section \verb#<Word>#. Contains a list of word probabilities
  {\tt P(w)}
  (estimated via your preferred method). It is used to compute
  observation probability toghether with the tag probabilities above.

  Each line is a word probability {\tt P(w)} with format {\tt word
  LogProbability}. A special line for \verb#<UNOBSERVED\_WORD># must be
  included.

  E.g.
   \begin{verbatim}
    afortunado -13.69500
    sutil -13.57721
    <UNOBSERVED_WORD> -13.82853
   \end{verbatim}
\end{itemize}

%..................................................
\subsection{Relaxation-Labelling Constraint Grammar File}
\label{file-relax}

   The syntax of the file is based on that of Constraint Grammars
   \cite{karlsson95}, but simplified in many aspects, and modified to 
   include weighted constraints.

   An initial file based on statistical constraints may be generated
   from a tagged corpus using the {\tt src/utilities/TRAIN}
   script provided with FreeLing.
   Later, hand written constraints can be added to the file to improve
   the tagger behaviour.

   The file consists of two sections: {\tt SETS} and {\tt CONSTRAINTS}.

   The {\tt SETS} section consists of a list of set definitions, each of the form
   {\tt Set-name = element1 element2 ... elementN ; }
    
    Where the {\tt Set-name} is any alphanumeric string starting with
    a capital letter, and the elements are either forms, lemmas, plain
    PoS tags, or senses. Forms are enclosed in parenthesis
    --e.g. \verb#(comimos)#--, lemmas in angle brackets
    --e.g. \verb#<comer>#--, PoS tags are alphanumeric strings
    starting with a capital letter --e.g. \verb#NCMS000#--, and senses
    are enclosed in square brackets --e.g. \verb#[00794578]#.  The
    sets must be homogeneous: That is, all the elements of a set have
    to be of the same kind.

   Examples of set definitions:\\
\begin{verbatim}
   DetMasc = DA0MS0 DA0MP0 DD0MS0 DD0MP0 DI0MS0 DI0MP0 DP1MSP DP1MPP
             DP2MSP DP2MPP DT0MS0 DT0MP0 DE0MS0 DE0MP0 AQ0MS0 AQ0MP0;
   VerbPron = <dar_cuenta> <atrever> <arrepentir> <equivocar> <inmutar>
              <morir> <ir> <manifestar> <precipitar> <referir> <reír> <venir>;
   Animal = [00008019] [00862484] [00862617] [00862750] [00862871] [00863425]
            [00863992] [00864099] [00864394] [00865075] [00865379] [00865569]
            [00865638] [00867302] [00867448] [00867773] [00867864] [00868028]
            [00868297] [00868486] [00868585] [00868729] [00911889] [00985200]
            [00990770] [01420347] [01586897] [01661105] [01661246] [01664986] 
            [01813568] [01883430] [01947400] [07400072] [07501137];
\end{verbatim}

   The {\tt CONSTRAINTS} section consists of a series of context
   constraits, each of the form: {\tt weight core context;}

    Where:
    \begin{itemize}
      \item {\tt weight} is a real value stating the compatibility (or
	incompatibility if negative) degree of the {\tt label} with the
	{\tt context}.
      \item {\tt core} indicates the analysis or
	analyses  (form interpretation) in a word that will 
        be affected by the constraint. It may be:
	\begin{itemize}
	\item Plain tag:  A plain complete PoS tag,  e.g. {\tt VMIP3S0}
	\item Wildcarded tag:  A PoS tag prefix, right-wilcarded, 
              e.g. {\tt VMI*}, {\tt VMIP*}. 
	\item Lemma: A lemma enclosed in angle brackets, optionaly
	  preceded by a tag or a wildcarded tag.
	  e.g.  \verb#<comer>#, \verb#VMIP3S0<comer>#,
	  \verb#VMI*<comer># will match any
	  word analysis with those tag/prefix and lemma.
	\item Form: Form enclosed in parenthesis, preceded by a PoS tag (or a
	  wilcarded tag).
	  e.g.  {\tt VMIP3S0(comi\'o)},  {\tt VMI*(comi\'o)} will match any
	  word analysis with those tag/prefix and form.
          Note that the form alone {\em is not} allowed in the rule core,
	  since the rule woull to distinguish among different analysis of
	  the same form.
	\item Sense: A sense code enclosed in square brackets, optionaly
	  preceded by a tag or a wildcarded tag.
	  e.g.  \verb#[00862617]#, \verb#NCMS000[00862617]#,
          \verb#NC*[00862617]# will match any
	  word analysis with those tag/prefix and sense.
	\end{itemize}
      \item {\tt context} is a list of conditions that the context of
        the word must satisfy for the constraint to be applied.
	Each condition is enclosed in parenthesis and the list (and
        thus the constraint) is finished with a semicolon.
	Each condition has the form: \\
	{\tt (position terms)} \\
	or either:\\
	{\tt (position terms barrier terms)} 
 
        Conditions may be negated using the token {\tt not},
         i.e. {\tt (not pos terms)}

	Where: 
	\begin{itemize}
	\item {\tt position} is the relative position where the condition
	  must be satisfied: -1 indicates the previous word and +1 the
	  next word. A position with a star (e.g. -2*) indicates that
	  any word is allowed to match starting from the indicated
	  position and advancing towards the beggining/end of the sentence.
 	\item {\tt terms} is a list of one or more terms separated by
 	the token {\tt or}. Each term may be:
    	   \begin{itemize}
  	     \item Plain tag:  A plain complete PoS tag,  e.g. {\tt VMIP3S0}
	     \item Wildcarded tag:  A PoS tag prefix, right-wilcarded, 
               e.g. {\tt VMI*}, {\tt VMIP*}. 
	     \item Lemma: A lemma enclosed in angle brackets, optionaly
	       preceded by a tag or a wildcarded tag.
	       e.g.  \verb#<comer>#, \verb#VMIP3S0<comer>#,
	       \verb#VMI*<comer># will match any
	       word analysis with those tag/prefix and lemma.
	     \item Form: Form enclosed in parenthesis, optionally
	       preceded by a PoS tag (or a wilcarded tag).
	       e.g.  {\tt (comi\'o)}, {\tt VMIP3S0(comi\'o)}, 
               {\tt VMI*(comi\'o)} will match any
	       word analysis with those tag/prefix and form.
               Note that --contrarily to when defining the rule core-- 
               the form alone {\em is} allowed in the context.
    	     \item Sense: A sense code enclosed in square brackets, optionaly
	       preceded by a tag or a wildcarded tag.
	       e.g.  \verb#[00862617]#, \verb#NCMS000[00862617]#, 
               \verb#NC*[00862617]# will match any
	       word analysis with those tag/prefix and sense.
    	     \item Set reference: A name of a previously defined {\em SET}
	       in curly brackets.
	       e.g.  \verb#{DetMasc}#, \verb#{VerbPron}#  will match any
	       word analysis with a tag, lemma or sense in the
	       specified set.
	   \end{itemize}

	\item {\tt barrier} states that the a match of the first term
	list is only acceptable if between the focus word and the
	matching word there is no match for the second term list.
	\end{itemize}
    \end{itemize}

    Note that the use of sense information in the rules of 
    the constraint grammar (either in the core or in the context) 
    only makes sense when this information distinguishes one analysis
    from another. If the sense tagging has been performed with the 
    option \verb#DuplicateAnalysis=no#, each PoS tag will have a list
    with all analysis, so the sense information will not distinguish
    one analysis from the other (there will be only one analysis with
    that sense, which will have at the same time all the other senses
    as well). 
    If the option \verb#DuplicateAnalysis# was active, the sense
    tagger duplicates the analysis, creating a new entry for each
    sense. So, when a rule selects an analysis having a certain sense,
    it is unselecting the other copies of the same analysis with 
    different senses.


	Examples:\\
	The next constraint states a high incompatibility for a word
	being a definite determiner ({\tt DA*}) if the next word is a personal form
	of a verb ({\tt VMI*}):\\
        {\tt -8.143  DA*  (1 VMI*); }
	
        The next constraint states a very high compatibility for the
        word {\sl mucho} (much) being an indefinite determiner ({\tt DI*}) 
        --and thus not being a pronoun or an adverb, or any
        other analysis it may have-- if the following word is a noun ({\tt NC*}):\\
        {\tt 60.0 DI* (mucho) (1 NC*);}

	The next constraint states a positive compatibility value for
	a word being a noun ({\tt NC*}) if somewhere to its left
	there is a determiner or an adjective ({\tt DA* or AQ*}), and
	between them there is not any other noun:\\
        {\tt 5.0 NC* (-1* DA* or AQ* barrier NC*);}

	The next constraint states a positive compatibility value for
	a word being a masculine noun ({\tt NCM*}) if the word to its
	left is a masculine determiner. It refers to a previously
	defined {\em SET} which should contain the list of all tags
	that are masculine determiners. This rule could be useful to
	correctly tag Spanish words which have two different NC
	analysis differing in gender: e.g. {\em el cura} (the priest)
	vs. {\em la cura} (the cure):\\
        {\tt 5.0 NCM* (-1* {DetMasc};}

	The next constraint adds some positive compatibility to a
	3rd person personal pronoun being of undefined gender and
	number ({\tt PP3CNA00}) if it has the possibility of being
        masculine singular ({\tt PP3MSA00}), the next word may have
        lemma {\sl estar} (to be), and the sencond word to the right
	is not a gerund ({\tt VMG}). This rule is intended to solve the 
        different behaviour of the Spanish word {\sl lo} in sentences 
        such as {\sl si, lo estoy} or {\sl lo estoy viendo}.\\
	{\tt 0.5 PP3CNA00 (0 PP3MSA00) (1 \verb#<estar>#) (not 2 VMG*);}



%..................................................
\section{Named Entity Classification Module}
\label{file-nec}

  The mission of the Named Entity Classification module is to assing a
  class to named entities in the text. It is a Machine-Learning based
  module, so the classes can be anything the model has been trained to
  recognize.

  When classified, the PoS tag of the word is changed to the label
  defined in the model.

  This module depends on a NER module being applied previously. If no
  entities are recognized, none can be classified.

  Models provided with FreeLing distinguish four classes: Person (tag
  {\tt NP00SP0}), Geographical location ({\tt NP00G00}), Organization
  ({\tt NP00O00}), and Others ({\tt NP00V00}).

  If you have an anotated corpus, the models can be trained using the
  scripts in {\tt src/utilities/nec}.  See the README there and the comments
  int the script for the details.

  The API of the class is the following:
\begin{verbatim}
class nec {
   public:
      /// Constructor
      nec(const std::string &, const std::string &); 

      /// Classify NEs in given sentence
      void analyze(std::list<sentence> &) const;
};
\end{verbatim}

  The constructor receives two parameters. The first is the tag that the
NER module assigned to Named Entities, so the NEC can know which words to classify.
 The second parameter is the prefix of the configuration files for the model, as described below.

\subsection{NEC Data File}

  The Named Entity Classification module requires three configuration
  files, with the same path and name, with suffixes {\tt .rgf}, {\tt
  .lex}, and {\tt .abm}.  Only the basename must be given as a
  parameter at instantiation time, file extensions are automatically added.

  The {\tt .abm} file contains an AdaBoost model based on shallow
  Decision Trees (see \cite{carreras03} for details). You don't need
  to understand this, unless you want to enter into the code of the
  AdaBoost classifier.

  The {\tt .lex} file is a dictionary that assigns a number to each
  symbolic feature used in the AdaBoost model. You don't need to
  understand this either unless you are a Machine Learning student or
  the like.

  Both {\tt .abm} and {\tt .lex} files may be generated from an
  annotated corpus using the training programs in {\tt libomlet} package.\\
  (see {\tt http://www.lsi.upc.edu/\~{\ }nlp/omlet+fries})

  The important file in the set is the {\tt .rgf} file. This contains
  a definition of the context features that must be extracted for each
  named entity.  The feature extraction language is that of
  \cite{roth04} with some useful extensions.

  If you need to know more about this (e.g. to develop a NE classifier
  for your language) please contact FreeLing authors.


%..................................................
\section{Chart Parser Module}

  The chart parser enriches each {\tt sentence} object with a {\tt
    parse\_tree} object, whose leaves have a link to the sentence
  words.

  The API of the parser is:
\begin{verbatim}
class chart_parser {
 public:
   /// Constructor
   chart_parser(const std::string&);
   /// Get the start symbol of the grammar
   std::string get_start_symbol(void) const;
   /// Parse sentences in list
   void analyze(std::list<sentence> &);
};
\end{verbatim}

  The constructor receives a file with the CFG grammar to be used by
  the grammar, which is described in the next section

  The method {\tt get\_start\_symbol} returns the initial symbol of the grammar, and
 is needed by the dependency parser (see below).

\subsection{Shallow Parser CFG file}
\label{file-cfg}

   This file contains a CFG grammar for the chart parser, and some
  directives to control which chart edges are selected to build the
  final tree.
  Comments may be introduced in the file, starting with ``\%'', the
  comment will finish at the end of the line.

   Grammar rules have the form: {\tt x ==> y, A, B.} 

   That is, the head of the rule is a non-terminal specified at the
   left hand side of the arrow symbol. The body of the rule is a
   sequence of terminals and nonterminals separated with commas and
   ended with a dot.

   Empty rules are not allowed, since they dramatically slow chart
   parsers. Nevertheless, any grammar may be written without empty
   rules (assuming you are not going to accept empty sentences).
  
   Rules with the same head may be or-ed using the bar symbol,
  as in: {\tt x ==> A, y | B, C.}

   The head component for the rule maybe specified prefixing it with a
   plus (+) sign, e.g.: {\tt nounphrase ==> DT, ADJ, +N, prepphrase. }.  
   If the head is not specified, the first symbol on
   the right hand side is assumed to be the head.  The head marks are
   not used in the chart parsing module, but are necessary for later
   dependency tree building.

   The grammar is case-sensitive, so make sure to write your terminals
  (PoS tags)  exactly as they are output by the tagger. Also, make
  sure that you capitalize your non-terminals in the same way
  everywhere they appear.

   Terminals are PoS tags, but some variations are allowed for
   flexibility:
   \begin{itemize}
     \item Plain tag:  A terminal may be a plain complete PoS tag,
     e.g. {\tt VMIP3S0}
     \item Wildcarding:  A terminal may be a PoS tag prefix,
     right-wilcarded, e.g. {\tt VMI*}, {\tt VMIP*}. 
    \item Specifying lemma: A terminal may be a PoS tag (or a
    wilcarded prefix) with a lemma enclosed in angle brackets,
     e.g  \verb#VMIP3S0<comer>#,  \verb#VMI*<comer># will match only
    words with those tag/prefix and lemma.
    \item Specifying form: A terminal may be a PoS tag (or a
    wilcarded prefix) with a form enclosed in parenthesis,
     e.g  {\tt VMIP3S0(comió)},  {\tt VMI*(comió)} will match only
    words with those tag/prefix and form.
    \item If a double-quoted string is given inside the angle brackets
      or parenthesis (for instance: \verb#VMIP3S0<"mylemmas.dat">#,
      or \verb#VMI*("myforms.dat")#) it is interpreted as a file
      name, and the terminal will match any lemma (or word form) found
      in that file.  If the file name is not an absolute path, it is
      interpreted as a relative path based at the location of the
      grammar file.
   \end{itemize}

   The grammar file may contain also some directives to help
   the parser decide which chart edges must be selected to build the
   tree.
   Directive commands start with the directive name (always prefixed
   with ``@''), followed by one or  more non-terminal symbols,
   separated with spaces. The list must end with a dot.
   \begin{itemize}
     \item {\tt @NOTOP} Non-terminal symbols listed under this
     directive will not be considered as valid tree roots, even if
     they cover the complete sentence.
     %% \item {\tt @ONLYTOP} Non-terminal symbols listed under this
     %% directive will be considered only if they are rooting a tree 
     %% covering the whole sentence.  (NOT IMPLEMENTED)
     \item {\tt @START} Specify which is the start symbol of the
       grammar. Exactly one non-terminal must be specified under this
       directive. 
       The parser will attempt to build a tree with this symbol as a
       root. If the result of the parsing is not a complete tree, or 
       no valid root nodes are found, a fictitious root node is
       created  with this label, and all created trees are attached to it.
     \item {\tt @FLAT} Subtrees for "flat" non-terminal symbols are flattened when
     the symbol is recursive. Only the highest occurrence appears 
     in the final parse tree.
     \item {\tt @HIDDEN} Non-teminal symbols specified under this
     directive will not appear in the final parse tree (their
     descendant nodes will be attached to their parent).
     \item {\tt @PRIOR} lists of non-terminal symbols in decreasing 
     priority order (the later in the list, the lower priority).
     When a top cell can be covered with two different non-terminals,
     the one with highest priority is chosen.  This has no effect
     on non-top cells (in fact, if you want that, your grammar
     is probably ambiguous and you should rethink it...)
   \end{itemize}

%..................................................

\section{Dependency Parser Module}

  The Txala dependency parser \cite{atserias05} gets parsed sentences --that is, {\tt sentence} objects which have been enriched with a {\tt parse\_tree} by the {\tt chart\_parser} (or by any other means).

\begin{verbatim}
class dep_txala : public dependency_parser {
 public:   
   /// constructor
   dep_txala(const std::string &, const std::string &);

   /// Enrich all sentences in given list with a depenceny tree.
   void analyze(std::list<sentence> &);
};
\end{verbatim}

  The constructor receives two strings: the name of the file containging the dependency rules to be used, and the start symbol of the grammar used by the {\tt chart\_parser} to parse the sentence.

   The dependency parser works in three stages:
\begin{itemize}
\item At the first stage, the
   \verb#<GRPAR># rules are used to complete the shallow parsing
   produced by the chart into a complete parsing tree.  The rules are
   applied to a pair of adjacent chunks. At each step, the selected
   pair is fused in a single chunk. The process stops when only one chunk remains

\item The next step is an automatic conversion of the complete parse tree to
  a dependency tree. Since the parsing grammar encodes information about the head of
  each rule, the conversion is straighforward

\item The last step is the labeling. Each edge in the dependeny tree is labeled with a 
  syntactic function, using the \verb#<GRLAB># rules
\end{itemize}

  The syntax and semantics of \verb#<GRPAR># and \verb#<GRLAB># rules are described in 
section \ref{file-dep}.

\subsection{Dependency Parsing Rule File}
\label{file-dep}

  The dependency rules file contains a set of rules to perform dependency parsing.

  The file consists of four sections:
  sections: \verb#<GRPAR>#, \verb#<GRLAB>#, \verb#<SEMDB>#, and \verb#<CLASS>#,
  respectively closed by tags \verb#</GRPAR>#, \verb#</GRLAB>#, \verb#</SEMDB>#, and \verb#</CLASS>#.

\begin{itemize} 
 \itemsep 0.2cm
    \item Section \verb#<GRPAR># contains rules to complete the
    partial parsing provided by the chart parser. The tree is
    completed by combining chunk pairs as stated by the rules. Rules
    are applied from highest priority (lower values) to lowest
    priority (higher values), and left-to right.
    That is, the pair of adjacent chunks matching the most prioritary
    rule is found, and the rule is applied, joining both chunks in
    one. The process is repeated until only one chunk is left.

    The rules can be enabled/disabled via the activation of global flags.
    Each rule may be stated to be enabled only if certain flags are on. 
    If none of its enabling flags are on, the rule is not applied.
    Each rule may also state which flags have to be toggled on/off after
    its application, thus enabling/disabling other rule subsets.

    Each line contains a rule, with the format:
\begin{verbatim}
priority flags context (ancestor,descendant) operation op-params flag-ops
\end{verbatim}
  where:
  \begin{itemize}
    \item \verb#priority# is a number stating the priority of a rule
      (the lower the number, the higher the priority).

    \item\verb#flags# is a list of strings separated by vertical bars
      (``\verb#|#'').  Each string is the name of a flag that will
      cause the rule to be enabled.  If \verb#enabling_flags# equals
      ``\verb#-#'', the rule will be always enabled.

    \item \verb#context# is a context limiting the application of the
      rule only to chunk pairs that are surrounded by the appropriate
      context (``\verb#-#'' means no limitations, and the rule is applied to
      any matching chunk pair) (see below).

    \item \verb#(ancestor,descendant)# are the labels of the adjacent pair of
    chunks the rule will be applied to. The labels are either assigned by
    the chunk parser, or by a \verb#RELABEL# operation on some other completion rule.
    The pair must be enclosed in parenthesis, separated by a comma, and 
    contain NO whitespaces.

    The chunk labels may be suffixed with one extra condition of the form:
    \verb#(form)#, \verb#<lemma>#, \verb#[class]#, or \verb#{PoS_regex}#.

    For instance,
 
    \begin{tabular}{|l|l|}
    The label: & Would match: \\ \hline\hline
    \verb#np#  & any chunk labeled \verb#np# by the chunker \\ \hline
    \verb#np(cats)# & any chunk labeled \verb#np# by the chunker \\
                    & with a head word with form \verb#cats# \\ \hline
    \verb#np<cat>#  & any chunk labeled \verb#np# by the chunker \\
                    & with a head word with lemma \verb#cat# \\ \hline
    \verb#np[animal]# & any chunk labeled \verb#np# by the chunker \\
                      & with a head word with a lemma in \verb#animal# \\
                      & category (see \verb#CLASS# section below) \\ \hline
    \verb#np{^N.M}# & any chunk labeled \verb#np# by the chunker \\
                    & with a head word with a PoS tag matching \\
                    & the \verb#^N.M# regular expression\\ \hline
    \end{tabular}

    \item \verb#operation# is the way in which \verb#ancestor#
    and \verb#descendant# nodes are to be combined (see below).

    \item The \verb#op-params# component has two meanings, depending
      on the \verb#operation# field: \verb#top_left# and
      \verb#top_right# operations must be followed by the literal
      \verb#RELABEL# plus the new label(s) to assign to the chunks.
      Other operations must be followed by the literal \verb#MATCHING# 
      plus the label to be matched.

      For \verb#top_left# and \verb#top_right# operations the labels
      following the keyword \verb#RELABEL# state the labels with which
      each chunk in the pair will be relabelled, in the format
      \verb#label1:label2#.  If specified, \verb#label1# will be the
      new label for the left chunk, and \verb#label2# the one for the
      right chunk. A dash ( ``\verb#-#'') means no relabelling. In none of
      both chunks is to be relabelled,  ``\verb#-#'' may be used instead of
       ``\verb#-:-#''.\\
      For example, the rule:\\
      \verb#20 - - (np,pp<of>) top_left RELABEL np-of:-  - #\
      will hang the \verb#pp# chunk as a daughter of the left chunk 
      in the pair (i.e. \verb#np#), then
      relabel the \verb#np# to \verb#np-of#, and leave the label for 
      the \verb#pp# unchanged.

       For \verb#last_left#, \verb#last_right# and
       \verb#cover_last_left# operations, the label following the keyword
       \verb#MATCHING# states the label that a node must have in
       order to be considered a valid ``last'' and get the subtree as
       a new child. This label may carry the same modifying suffixes
       than the chunk labels. If no node with this label is found in
       the tree, the rule is not applied.\\
      For example, the rule:\\
      \verb#20 - - (vp,pp<of>) last_left MATCHING np -#\\
      will hang the \verb#pp# chunk as a daughter of the last subtree 
      labeled \verb#np# found inside the \verb#vp# chunk.

   \item The last field \verb#flag-ops# is a space-separated list of
     flags to be toggled on/off.  The list may be empty (meaning that
     the rule doesn't change the status of any flag).  If a flag name
     is preceded by a ``\verb#+#'', it will be toggled on. If the leading
     symbol is a ``\verb#-#'', it will be toggled off.

  \end{itemize}

  For instance, the rule:
\begin{verbatim}
  20 - - (np,pp<of>) top_left RELABEL - -
\end{verbatim}

  states that if two subtrees labelled \verb#np# and \verb#pp# are
  found contiguous in the partial tree, and the second head word has
  lemma \verb#of#, then the later (rightmost) is added as a new child
  of the former (leftmost), whatever the context is, without need of
  any special flag active, and performing no relabelling of the new
  tree root.

   The supported tree-building operations are the following:
  \begin{itemize}
   \item \verb#top_left#: The right subtree is added as a daughter of
     the left subtree. The root of the new tree is the root of the
     left subtree. If a \verb#label# value other than ``\verb#-#'' is
     specified, the root is relabelled with that string.
   \item \verb#last_left#: The right subtree is added as a daughter of
     the last node inside the left subtree matching \verb#label# value
     (or to the root if none is found). The root of the new tree is
     the root of the left subtree.
   \item \verb#top_right#: The left subtree is added as a new daughter
     of the right subtree. The root of the new tree is the root of the
     right subtree. If a \verb#label# value other than ``\verb#-#'' is
     specified, the root is relabelled with that string.
   \item \verb#last_right#: The left subtree is added as a daughter of
     the last node inside the right subtree matching \verb#label#
     value (or to the root if none is found). The root of the new tree
     is the root of the right subtree.
   \item \verb#cover_last_left#: The left subtree ($s$) takes the
     position of the last node ($x$) inside the right subtree matching
     \verb#label# value. The node $x$ is hanged as new child of $s$.
     The root of the new tree is the root of the right subtree.
  \end{itemize}

  The context may be specified as a sequence of chunk labels,
  separated by underscores ``\verb#_#''.
   One of the chunk labels must be \verb#$$#, and refers to the pair of chunks
  which the rule is being applied to.

  For instance, the rule:
\begin{verbatim}
   20 - $$_vp (np,pp<of>) top_left RELABEL -
\end{verbatim}

  would add the rightmost chunk in the pair (\verb#pp<of>#) under the
  leftmost (\verb#np#) only if the chunk immediate to the right of the pair
  is labeled \verb#vp#.

  Other admitted labels in the context are: \verb#?# (matching exactly
  one chunk, with any label), \verb#*# (matching zero or more chunks
  with any label), and \verb#OUT# (matching a sentence boundary).

  For instance the context \verb#np_$$_*_vp_?_OUT# would match a
  sentence in which the focus pair of chunks is immediately after an
  \verb#np#, and the second-to-last chunk is labeled \verb#vp#.

  Context conditions can be globally negated preceding them with an exclamation
  mark (\verb#!#). E.g. \verb#!np_$$_*_vp# would cause the rule to be applied only
  if that particular context is {\em not satisfied}.

  Context condition components may also be individually negated
  preceding them with the symbol \verb#~#. E.g. the rule
  \verb#np_$$_~vp# would be satisfied if the preceding chunk is
  labeled \verb#np# and the following chunk has any label but
  \verb#vp#.

  Enabling flags may be defined and used at the grammarian's will. 
  For instance, the rule:
\begin{verbatim}
20 INIT|PH1 $$_vp (np,pp<of>) last_left MATCHING npms[animal] +PH2 -INIT -PH1
\end{verbatim}

   Will be applied if either \verb#INIT# or \verb#PH1# flags are
   on, the chunk pair is a \verb#np# followed by a \verb#pp# with head
   lemma \verb#of#, and the context (one \verb#vp# chunk following the
   pair) is met. Then, the deepest rightmost node matching the label
   \verb#npms[animal]# will be sought in the left chunk, and the right
   chunk will be linked as one of its children. If no such node is found,
   the rule will not be applied. 
   
    After applying the rule, the flag \verb#PH2# will be toggled
    on, and the flags \verb#INIT# and \verb#PH1# will be toggled
    off.

    The only predefined flag is \verb#INIT#, which is toggled on when
    the parsing starts.  The grammarian can define any alphanumerical
    string as a flag, simply toggling it on in some rule.

\item Section \verb#<GRLAB># contains two kind of lines.

  The first kind are the lines defining \verb#UNIQUE# labels, which
  have the format:
\begin{verbatim}
  UNIQUE label1 label2 label3 ...
\end{verbatim}

  You can specify many \verb#UNIQUE# lines, each with one or more
  labels. The effect is the same than having all of them in a single
  line, and the order is not relevant.

  Labels in \verb#UNIQUE# lists will be assigned only once per
  head. That is, if a head has a daugther with a dependency already
  labeled as \verb#label1#, rules assigning this label will be ignored
  for all other daugthers of the same head. (e.g. if a verb has got a
  \verb#subject# label for one of its dependencies, no other
  dependency will get that label, even if it meets the conditions to
  do so).

   The second kind of lines state the rules to label the
  dependences extracted from the full parse tree build with the
  rules in previous section:
  
  Each line contains a rule, with the format:
\begin{verbatim}
  ancestor-label dependence-label condition1 condition2 ...
\end{verbatim}

  where:
  \begin{itemize}
    \item  \verb#ancestor-label# is the label of the node which is
    head of the dependence.
    \item \verb#dependence-label# is the label to be assigned to the dependence
    \item \verb#condition# is a list of conditions that the dependence
    has to match to satisfy the rule.
  \end{itemize}

   Each \verb#condition# has one of the forms:
\begin{verbatim}
  node.attribute = value
  node.attribute != value
\end{verbatim}

   Where \verb#node# is a string describing a node on which the \verb#attribute# has to be checked.
   The \verb#value# is a string to be matched, or a set of strings (separated by ``\verb#|#''). The strings can be right-wildcarded (e.g. \verb#np*# is allowed, but not \verb#n*p#. For the \verb#pos# attribute, \verb#value# can be any valid regular expression.

   The \verb#node# expresses a path to locate the node to be checked. The path must start with \verb#p# (parent node) or \verb#d# (descendant node), and may be followed by a colon-separated list of labels. For instance \verb#p:sn:n# refers to the first node labeled \verb#n# found under a node labeled \verb#sn# which is under the dependency parent \verb#p#. 
 
   Possible {\tt attribute} to be used:
  \begin{itemize}
    \item \verb#label#: chunk label (or PoS tag) of the node.
    \item \verb#side#: (left or right) position of the specified node with respect to the other.
    \item \verb#lemma#: lemma of the node head word.
    \item \verb#pos#: PoS tag of the node head word
    \item \verb#class#: word class (see below) of lemma of the node head word.
    \item \verb#tonto#: EWN Top Ontology properties of the node head word.
    \item \verb#semfile#: WN semantic file of the node head word.
    \item \verb#synon#: Synonym lemmas of the node head word (according to WN).
    \item \verb#asynon#: Synonym lemmas of the node head word ancestors (according to WN).
  \end{itemize}

  Note that since no disambiguation is required, the attributes dealing with semantic properties will be satisfied if any of the word senses matches the condition.

   For instance, the rule:
\begin{verbatim}
verb-phr    subj    d.label=np*      d.side=left
\end{verbatim}
  states that if a \verb#verb-phr# node has a daughter to its left, with a label
  starting by \verb#np#, this dependence is to be labeled as \verb#subj#.
 
   Similarly, the rule:
\begin{verbatim}
verb-phr    obj    d.label=np*  d:sn.tonto=Edible  p.lemma=eat|gulp
\end{verbatim}
  states that if a \verb#verb-phr# node has {\tt eat} or {\tt gulp} as
  lemma, and a descendant with a label starting by \verb#np# and containing
  a daughter labeled \verb#sn# that has {\tt Edible} property in EWN
  Top ontology, this dependence is to be labeled as \verb#obj#.

 
   \item Section \verb#<SEMDB># is only necessary if the dependency labeling rules in section \verb#<GRLAB># use conditions on semantic values (that is, any of \verb#tonto#, \verb#semfile#, \verb#synon#, or \verb#asynon#).  Since it is needed by \verb#<GRLAB># rules, section \verb#<SEMDB># must be defined {\em before} section \verb#<GRLAB>#.
  The section must contain two lines specifying two semantic information files, a {\tt SenseFile} and a {\tt WNFile}. The filenames may be absolute or relative to the location of the dependency rules file.
\begin{verbatim}
<SEMDB>
SenseFile ../senses16.db
WNFile    ../../common/wn16.db
</SEMDB>
\end{verbatim}

   The \verb#SenseFile# must be a BerkeleyDB indexed file as described in the \ref{senses-file} section.
   The \verb#WNFile# must be a BerkeleyDB indexed file, containing ontology information (hyperonymy, Top Ontology, WN semantic File, etc. The contents of this file are described in section \ref{file-wn}.

   \item Section \verb#<CLASS># contains class definitions which may
   be used as attributes in the dependency labelling rules.

   Each line contains a class assignation for a lemma, with two possible formats:
\begin{verbatim}
  class-name  lemma      comments
  class-name  "filename"   comments
\end{verbatim}

   For instance, the following lines assign to the class \verb#mov#
   the four listed verbs, and to the class \verb#animal# all lemmas
   found in \verb#animals.dat# file.  In the later case, if the file
   name is not an absolute path, it is interpreted as a relative path
   based at the location of the rule file.

    Anything to the right of the second field is considered a comment and ignored.
\begin{verbatim}
mov     go      prep= to,towards
mov     come    prep= from
mov     walk    prep= through
mov     run     prep= to,towards   D.O.

animal "animals.dat"
\end{verbatim}

\end{itemize}

%..................................................
\section{Coreference Resolution Module}

  This module is a machine-learning based coreference solver,
  following the algorithm proposed by \cite{soon01}.  It takes a
  document parsed by the shallow parser to detect noun phrases, and
  decides which noun phrases are coreferential.

  The api of the module is the following:
\begin{verbatim}
class coref {
   public:
    /// Constructor
    coref(const std::string &, const int);

    /// Classify in coreference chains noun phrases in given document
    void analyze(document &) const;
};
\end{verbatim}

  The parameters received by the constructor are a filename, and an
  integer bitmask specifying which attributes have to be used by the
  classifier.

  The meaning of the attributes can be found in the source file {\tt include/freeling/coref\_fex.h}. If you just want to use the module, set the value of this parameter to 0xFFFFFF to select all the attributes.

  The string parameter is the name of the configuration file, which is described below:

\subsection{Coreference Solver configuration file}

  The Coreference Solver module reads this file to find out some needed parameters.  The file has three sections:
\begin{itemize}
   \item Section \verb#<ABModel># path to the file containing the
     trained AdaBoost model.  The {\tt .abm} file contains an AdaBoost
     model based on shallow Decision Trees (see \cite{carreras03} for
     details). You don't need to understand this, unless you want to
     enter into the code of the AdaBoost classifier.

     The name of the file may be either absolute or relative to the 
     location of the Coreference Solver config file.\\
     e.g:
\begin{verbatim}
<ABModel>
coref.abm
</ABModel>
\end{verbatim}

     It may be generated from an annotated corpus using the training
     programs in {\tt src/utilities/coref}, which use the libomlet
     package. (see {\tt http://www.lsi.upc.edu/\~{\ }nlp/omlet+fries})

     If you need to know more about this (e.g. to develop a
     Coreference Solver for your language) please contact FreeLing
     authors.

   \item Section \verb#<SemDB># specifies the files that contain a
    semantic database. This is required to compute some WN-based attributes
    used by the solver.  

    The section must contain two lines specifying two semantic
    information files, a {\tt SenseFile} and a {\tt WNFile}. The
    filenames may be absolute or relative to the location of the
    dependency rules file. For example:
\begin{verbatim}
<SEMDB>
SenseFile ../senses16.db
WNFile    ../../common/wn16.db
</SEMDB>
\end{verbatim}

   \item Section \verb#<MaxDistance># states the maximum distance (in
     words) at which possible corefernces will be considered. Short
     values will cause the solver to miss distant coreferents.
     Long distances will introduce a huge amount of possible coreferent 
     candidate pairs, slow the system, and produce a larger amount of
     false positives.
\end{itemize}

%..................................................
\section{Semantic Database Module}

  This module is not a component in the default analysis chain, but it
  can be used by the applications to enrich or post process the results of 
  the analysis.
  
   Moreover, this module is used by the other modules that need access
  to the semantic database: The sense annotator {\tt senses}, the
  dependency parser {\tt dep\_txala}, and the coreference solver {\tt coref}.

   The API for this module is 

\begin{verbatim}
class semanticDB {
   public:
      /// Constructor
      semanticDB(const std::string &, const std::string &); 
 
      /// get list of words for a sense+pos
      std::list<std::string> get_sense_words(const std::string &, const std::string &);

      /// get list of senses for a lemma+pos
      std::list<std::string> get_word_senses(const std::string &, const std::string &);

      /// get sense info for a sensecode+pos
      sense_info get_sense_info(const std::string &, const std::string &);
};
\end{verbatim}


\subsection{Sense Dictionary File}
\label{file-sense}

  The sense dictionary file is a Berkeley DB indexed file.

  It can be created with the {\tt indexdict} program provided with
  FreeLing, which is called with the command:
  \begin{verbatim}
   indexdict indexed-dict-name  <source-dict 
  \end{verbatim}
  See the (very simple) source code in {\tt src/main/utilities/indexdict.cc}
  if you're interested on how it is indexed.

  The source file (e.g. {\tt senses16.src} provided with FreeLing)
  must contain the sense list of each lemma--PoS, one entry per line.

  Each line has format: {\tt type:lemma:PoS synset1 synset2 ...}.\\
  E.g. \\
  \verb#W:cebolla:N 05760066 08734429 08734702#\\
  \verb#S:07389783:N chaval chico joven mozo muchacho#\\
  
  The {\em type} field may be either {\tt W} (for Word) or {\tt S}
  (for Sense), and indicates whether the rest of the line contains
  either a word and all its sense codes, or a sense code and all
  its synonym words.

  For {\tt W} entries, the sense code list is assumed to be ordered
  from most to least frequent sense for that lemma--PoS by the sense
  annotation module.  This is used when value {\tt msf} is
  selected for the {\tt SenseAnnotation} option.

  Type {\tt S} entries are used by dependency parsing rules.

  Sense codes can be anything (assuming your later processes know what
  to do with them). The provided files contain WordNet 1.6 synset
  codes.


\subsection{WordNet file}
\label{file-wn}

  The WordNet file is a Berkeley DB indexed file.

  It can be created with the {\tt indexdict} program provided with
  FreeLing, which is called with the command:
  \begin{verbatim}
   indexdict indexed-wn-name  <source-wn
  \end{verbatim}
  See the (very simple) source code in {\tt src/main/utilities/indexdict.cc}
  if you're interested on how it is indexed.

  The source file (e.g. {\tt wn16.src} provided with FreeLing)
  must contain at each line the information relative to a sense, 
  with the following format:
\begin{verbatim}
synset:PoS  hypern:hypern:...:hypern  semfile  TopOnto:TopOnto:...:TopOnto
\end{verbatim}

 That is: the first field is the synset code plus its PoS, separated by a colon. The second field is a colon-separated list of its hypernym synsets. The third field is the WN semantic file the synset belongs to, and the last field is a colon-separated list of EuroWN TopOntology codes valid for the synset.

  
Note that the only relation encoded here is hypernymy. Note also that semantic codes such as WN semantic file or EWN TopOntology features are simply lists of strings. Thus, you can include in this file any ontological or semantic information you need, just substituing the WN-related codes by your own semantic categories.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Using the library from your own application}

 The library may be used to develop your own NLP application (e.g. a
 machine translation system, an intelligent indexation module for a
 search engine, etc.)

  To achieve this goal you have to link your application to the
 library, and access it via the provided API.  Currently, the library
 provides a complete C++ API, a quite-complete Java API, and 
 half-complete perl and python APIs.

%..................................................
 \section{Basic Classes}

 This section briefs the basic C++ classes any application needs to
know. For detailed API definition, consult the technical documentation
in {\tt doc/html} and {\tt doc/latex} directories.

\subsection{Linguistic Data Classes}
\label{ssec-data}

  The different processing modules work on objects containing
  linguistic data (such as a word, a PoS tag, a sentence...).

   Your application must be aware of those classes in order to
  be able to provide to each processing module the right data,
  and to correctly interpret the module results.

   The Linguistic Data classes are defined in {\tt libfries} library. Refer 
  to the documentation in that library for the details on the classes.

   The linguistic classes are:
\begin{itemize}
\itemsep 0cm
\item {\tt analysis}: A tuple  \verb#<lemma, PoS tag, probability, sense list>#
\item {\tt word}:     A word form with a list of possible analysis.
\item {\tt sentence}: A list of words known to be a complete
  sentence. A sentence may have associated a {\tt parse\_tree} object and a {\tt dependency\_tree}.
\item {\tt parse\_tree}: An {\it n}-ary tree where each node contains
  either a non-terminal label, or --if the node is a leaf-- a pointer
  to the appropriate {\tt word} object in the sentence the tree
  belongs to.
\item {\tt dep\_tree}: An {\it n}-ary tree where each node contains a 
  reference to a node in a {\tt parse\_tree}. The structure of the {\tt dep\_tree}
  establishes syntactic dependency relationships between sentence constituents.
\end{itemize}

\subsection{Processing modules}

  The main processing classes in the library are:
\begin{itemize}
\itemsep 0cm
\item {\tt tokenizer}: Receives plain text and returns a list of {\tt word} objects.
\item {\tt splitter}: Receives a list of {\tt word} objects and
  returns a list of {\tt sentence} objects.
\item {\tt maco}: Receives a list of {\tt sentence} objects and
  morphologically annotates each {\tt word} object in the given
  sentences. Includes specific submodules (e.g, detection of date,
  number, multiwords, etc.) which can be activated at will.
\item {\tt tagger}: Receives a list of {\tt sentence} objects and
  disambiguates the PoS of each {\tt word} object in the given
  sentences.
\item {\tt parser}: Receives a list of {\tt sentence} objects and
  associates to each of them a {\tt parse\_tree} object.
\item {\tt dependency}: Receives a list of parsed {\tt sentence}
 objects and associates to each of them a {\tt dep\_tree} object.
\item {\tt coref}: Receives a document (containing a list of parsed
  {\tt sentence} objects) and labels each noun phrase as belonging
  to a {\em coreference group}, if appropriate.
\end{itemize}

  You may create as many instances of each as you need. 
  Constructors for each of them receive the appropriate options
  (e.g. the name of a dictionary, hmm, or grammar file), so you can 
  create each instance with the required capabilities (for instance,
  a tagger for English and another for Spanish).

%..................................................
\section{Sample programs}
\label{sec-main}

 The directory {\tt src/main/simple\_examples} in the tarball contains 
 some example programs to illustrate how to call the library.

 See the README file in that directory for details on what does each of the 
 programs.

 The most complete program in that directory is {\tt sample.cc}, which
 is very similar to the program depicted below, which
 reads text from stdin, morphologically analyzes it, and processes 
 the obtained results. 

  Note that depending on the application, the input text
  could be obtained from a speech recongnition system, or from a 
  XML parser, or from any source suiting the application goals. 
  Similarly, the obtained analysis, instead of being output, could
  be used in a translation system, or sent to a dialogue control module, etc.

{\footnotesize
\begin{verbatim}
int main() {
  string text;
  list<word> lw;
  list<sentence> ls;

  string path="/usr/local/share/FreeLing/es/";

  // create analyzers
  tokenizer tk(path+"tokenizer.dat"); 
  splitter sp(path+"splitter.dat");
  
  // morphological analysis has a lot of options, and for simplicity they are packed up
  // in a maco_options object. First, create the maco_options object with default values.
  maco_options opt("es");  
  // then, set required options on/off  
  opt.QuantitiesDetection = false;  //deactivate ratio/currency/magnitudes detection 
  opt.AffixAnalysis = true; opt.MultiwordsDetection = true; opt.NumbersDetection = true; 
  opt.PunctuationDetection = true; opt.DatesDetection = true; opt.QuantitiesDetection = false; 
  opt.DictionarySearch = true; opt.ProbabilityAssignment = true; opt.NERecognition = NER_BASIC;   
  // alternatively, you can set active modules in a single call:
  //     opt.set_active_modules(true, true, true, true, true, false, true, true, NER_BASIC);

  // and provide files for morphological submodules. Note that it is not necessary
  // to set opt.QuantitiesFile, since Quantities module was deactivated.
  opt.LocutionsFile=path+"locucions.dat"; opt.AffixFile=path+"afixos.dat";
  opt.ProbabilityFile=path+"probabilitats.dat"; opt.DictionaryFile=path+"maco.db";
  opt.NPdataFile=path+"np.dat"; opt.PunctuationFile=path+"../common/punct.dat"; 
  // alternatively, you can set the files in a single call:
  //  opt.set_data_files(path+"locucions.dat", "", path+"afixos.dat", 
  //                     path+"probabilitats.dat", path+"maco.db", 
  //                     path+"np.dat", path+"../common/punct.dat");
  
  // create the analyzer with the just build set of maco_options
  maco morfo(opt); 
  // create a hmm tagger for spanish (with retokenization ability, and forced 
  // to choose only one tag per word)
  hmm_tagger tagger("es", path+"tagger.dat", true, true); 
  // create chunker
  chart_parser parser(path+"grammar-dep.dat");
  // create dependency parser 
  dep_txala dep(path+"dep/dependences.dat", parser.get_start_symbol());
  
  // get plain text input lines while not EOF.
  while (getline(cin,text)) {
    
    // tokenize input line into a list of words
    lw=tk.tokenize(text);
    
    // accumulate list of words in splitter buffer, returning a list of sentences.
    // The resulting list of sentences may be empty if the splitter has still not 
    // enough evidence to decide that a complete sentence has been found. The list
    // may contain more than one sentence (since a single input line may consist 
    // of several complete sentences).
    ls=sp.split(lw, false);
    
    // perform and output morphosyntactic analysis and disambiguation
    morfo.analyze(ls);
    tagger.analyze(ls);

    // Do whatever our application does with the analyzed sentences
    ProcessResults(ls);
    
    // clear temporary lists;
    lw.clear(); ls.clear();    
  }
  
  // No more lines to read. Make sure the splitter doesn't retain anything  
  ls=sp.split(lw, true);   
 
  // analyze sentence(s) which might be lingering in the buffer, if any.
  morfo.analyze(ls);
  tagger.analyze(ls);

  // Process last sentence(s)
  ProcessResults(ls);
}
\end{verbatim}
}

 The processing performed on the obtained results would obviously
 depend on the goal of the application (translation, indexation,
 etc.). In order to illustrate the structure of the linguistic data
 objects, a simple procedure is presented below, in which the processing
 consists of merely printing the results to stdout in XML format.

{\footnotesize
\begin{verbatim}

void ProcessResults(const list<sentence> &ls) {
  
  list<sentence>::const_iterator s;
  word::const_iterator a;   //iterator over all analysis of a word
  sentence::const_iterator w;
  
  // for each sentence in list
  for (s=ls.begin(); s!=ls.end(); s++) {
    
    // print sentence XML tag
    cout<<"<SENT>"<<endl;
      
    // for each word in sentence
    for (w=s->begin(); w!=s->end(); w++) {
      
      // print word form, with PoS and lemma chosen by the tagger
      cout<<"  <WORD form=\""<<w->get_form();
      cout<<"\" lemma=\""<<w->get_lemma();
      cout<<"\" pos=\""<<w->get_parole();
      cout<<"\">"<<endl;
      
      // for each possible analysis in word, output lemma, parole and probability
      for (a=w->analysis_begin(); a!=w->analysis_end(); ++a) {
	
        // print analysis info
        cout<<"    <ANALYSIS lemma=\""<<a->get_lemma();
        cout<<"\" pos=\""<<a->get_parole();
        cout<<"\" prob=\""<<a->get_prob();
        cout<<"\"/>"<<endl;
      }
      
      // close word XML tag after list of analysis
      cout<<"</WORD>"<<endl;
    }
    
    // close sentence XML tag
    cout<<"</SENT>"<<endl;
  }
}  
\end{verbatim}
}

 The above sample program may be found in {\tt /src/main/simple\_examples/sample.cc}
 in FreeLing tarball.

 Once you have compiled and installed FreeLing, you can build this
 sample program (or any other you may want to write) with the command:\\
 {\tt g++ -o sample sample.cc -lmorfo -ldb\_cxx -lpcre -lomlet -fries}

 Check the README file in the directory to learn more about compiling and using
 the sample programs.

 Option {\tt -lmorfo} links with libmorfo library, which is the final result of the
 FreeLing compilation process.  The oher options refer to other
 libraries required by FreeLing.

   You may have to add some  {\tt -I} and/or  {\tt -L} options to the
 compilation command depending on where the headers and code of
 required libraries are located. For instance, if you installed some
 of the libraries in {\tt /usr/local/mylib} instead of the default 
place {\tt /usr/local}, you'll have to add the options 
 {\tt -I/usr/local/mylib/include -L/usr/local/mylib/lib} 
to the command above.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Using the sample main program to process corpora}
\label{cap-analyzer}

 The simplest way to use the FreeLing libraries is via the provided
 {\tt analyzer} sample main program, which allows the user to process an input text
 to obtain several linguistic processings.

  Since it is impossible to write a program that fits
  everyone's needs, the {\tt analyzer} program offers you almost all
  functionalities included in FreeLing, but if you want it to output
  more information, or do so in a specific format, or combine the
  modules in a different way, the right path to follow is building
  your own main program or adapting one of the existing, as described
  in section \ref{sec-main}

  FreeLing provides also a couple of programs {\tt analyzer\_server}
  and {\tt analyzer\_client} that perform the same task, but the
  server remains loaded after analyzing each client's request, thus
  reducing the starting-up overhead if many small files have to be
  processed.

  To ease the invocation of the program, a script named {\tt analyze}
  (no final {\tt r}) is provided. This is script is able to locate
  default configuration files, define library search paths, and 
  decide whether you want the client-server or the staight version.

 The sample main program is called with the command:
\begin{verbatim}
 analyze [channel-name] [-f config-file] [options]
\end{verbatim}

  If \verb#channel-name# is ommited, the analyzer is started in straight mode:
  It will load the configuration, read input from stdin, and write results to stdout.\\
E.g.:
\begin{verbatim}
 analyze -f en.cfg  <myinput  >myoutput
\end{verbatim}

  When the input file ends, the analyzer will stop and will have to be
  reloaded again to process a new file.

  If \verb#config-file# is not specified, a file named 
 {\tt analyzer.cfg} is searched in the current working 
 directory. 
  If it is specified but not located in the current directory, it will be searched
 in FreeLing installation directory ({\tt /usr/local/share/FreeLing/config} by default).

  Extra options may be specified in the command line to override any settings in
  \verb#config-file#. See section \ref{ss-options} for details.
  

  If \verb#channel-name# is specified (any valid filename is acceptable),
 a server is initiated, and named pipes with the given name are created.\\
E.g.:
\begin{verbatim}
 analyze mychannel -f en.cfg  <myinput  >myoutput
\end{verbatim}

  Then, clients can request analysis to the server, with:
\begin{verbatim}
 analyzer_client mychannel  <myinput  >myoutput
\end{verbatim}

  When the server ends attending one request, a new client can be
  started, and the server will be ready to analyze it, without having
  to reload the analyzers.

  The server doesn't take care of syncronizing with the clients, which means
 that if two clients send data to the same server, their inputs will intermix
 in the output.

  The user starting the clients must ensure there is no overlapping in the requests.

%..................................................
\section{Usage example}

   Assuming we have the folowing input file {\tt mytext.txt}:
{\small {\tt 
\begin{center}
\begin{tabular}{p{6cm}}
El gato come pescado. Pero a Don Jaime no le gustan los gatos.
\end{tabular}
\end{center}
}}

\noindent we could issue the command:
\begin{verbatim}
  analyze -f myconfig.cfg <mytext.txt >mytext.mrf
\end{verbatim}
   Assuming that {\tt myconfig.cfg} is the file presented in
   section~\ref{ss-config}. Given the options there, the produced
   output would correspond to {\tt morfo} format (i.e. morphological
   analysis but no PoS tagging). The expected results are:
{\small
\begin{verbatim}
 El el DA0MS0 1 
 gato gato NCMS000 1 
 come comer VMIP3S0 0.75 comer VMM02S0 0.25 
 pescado pescado NCMS000 0.833333 pescar VMP00SM 0.166667 
 . . Fp 1 

 Pero pero CC 0.99878 pero NCMS000 0.00121951 Pero NP00000 0.00121951 
 a a NCFS000 0.0054008 a SPS00 0.994599 
 Don_Jaime Don_Jaime NP00000 1 
 no no NCMS000 0.00231911 no RN 0.997681 
 le él PP3CSD00 1 
 gustan gustar VMIP3P0 1 
 los el DA0MP0 0.975719 lo NCMP000 0.00019425 él PP3MPA00 0.024087 
 gatos gato NCMP000 1 
 . . Fp 1 

\end{verbatim}
}

\noindent If we also wanted PoS tagging, we could have issued the command:
\begin{verbatim}
  analyze -f myconfig.cfg --outf tagged <mytext.txt >mytext.tag
\end{verbatim}
\noindent  to obtain the tagged output:
{\small 
\begin{verbatim}
 El el DA0MS0
 gato gato NCMS000
 come comer VMIP3S0
 pescado pescado NCMS000
 . . Fp

 Pero pero CC
 a a SPS00
 Don_Jaime Don_Jaime NP00000
 no no RN
 le él PP3CSD00
 gustan gustar VMIP3P0
 los el DA0MP0
 gatos gato NCMP000
 . . Fp

\end{verbatim}
}

\noindent We can also ask for the synsets of the tagged words:
\begin{verbatim}
  analyze -f myconfig.cfg --outf sense --sense all  <mytext.txt >mytext.sen
\end{verbatim}
\noindent obtaining the output:
{\small 
\begin{verbatim}
 El el DA0MS0
 gato gato NCMS000 01630731:07221232:01631653
 come comer VMIP3S0 00794578:00793267
 pescado pescado NCMS000 05810856:02006311
 . . Fp

 Pero pero CC
 a a SPS00
 Don_Jaime Don_Jaime NP00000
 no no RN
 le él PP3CSD00
 gustan gustar VMIP3P0 01244897:01213391:01241953
 los el DA0MP0
 gatos gato NCMP000 01630731:07221232:01631653
 . . Fp

\end{verbatim}
}

   Alternatively, if we don't want to repeat the first steps that we
   had already performed, we could use the output of the morphological
   analyzer as input to the tagger:
\begin{verbatim}
analyzer -f myconfig.cfg --inpf morfo --outf tagged <mytext.mrf >mytext.tag
\end{verbatim}
 
   See options InputFormat and OutputFormat in
   section~\ref{ss-options} for details on which are valid input and
   output formats.

%..................................................
\section{Configuration File and Command Line Options}

   Almost all options may be specified either in the configuration
   file or in the command line, having the later precedence over the
   former.

   Valid options are presented in section~\ref{ss-options}, both in
   their command-line and configuration file notations. Configuration
   file follows the usual linux standards, a sample file may be seen
   in section~\ref{ss-config}.

  The FreeLing package includes default configuration files. They can
  be found at the directory {\tt share/FreeLing/config} under the FreeLing
  installation directory (by default, {\tt /usr/local}).
   The {\tt analyze} script will try to locate the configuration file in
  that directory if it is not found in the current working directory.


%······························································
\subsection{Valid options}
\label{ss-options}

   This section presents the options that can be given to the {\tt
     analyzer} program (and thus, also to the {\tt analyzer\_server}
   program and to the {\tt analyze} script). All options can be
   written in the configuration file as well as in the command line.
   The later has always precedence over the former.

\begin{itemize}

\item {\bf Help}

\begin{tabular}{|l|l|}
Command line       & Configuration file   \\ \hline
\verb#-h#, \verb#--help#   &  {\tt N/A}   \\ \hline
\end{tabular}

 Prints to stdout a help screen with valid options and exits.

\item {\bf Configuration file}

\begin{tabular}{|l|l|}
Command line       & Configuration file   \\ \hline
\verb#-f <filename>#  &  {\tt N/A}        \\ \hline 
\end{tabular}

 Specify configuration file to use (default: analyzer.cfg).

\item {\bf Trace Level}

\begin{tabular}{|l|l|}
Command line       & Configuration file   \\ \hline
\verb#-l <int>#, \verb#--tlevel <int>#   & \verb#TraceLevel=<int>#  \\ \hline
\end{tabular}

 Set the trace level (0 = no trace, higher values = more trace), for
 debugging purposes.

 This will work only if the library was compiled with tracing information, 
 using {\tt ./configure --enable-traces}.
 Note that the code with tracing information is slower than the code 
 compiled without it, even when traces are not active.

\item {\bf Trace Module}

\begin{tabular}{|l|l|}
Command line       & Configuration file   \\ \hline
\verb#-m <mask>#, \verb#--tmod <mask>#  & \verb#TraceModule=<mask>#  \\ \hline
\end{tabular}

  Specify modules to trace. Each module is identified with an hexadecimal flag.
 All flags may be OR-ed to specificy the set of modules to be traced.

 Valid masks are:

\begin{tabular}{ll}
   {\bf Module}           & {\bf Mask}  \\ \hline
Splitter               & 0x00000001 \\
Tokenizer              & 0x00000002 \\
Morphological analyzer & 0x00000004 \\
Options management     & 0x00000008 \\
Number detection       & 0x00000010 \\
Date identification         & 0x00000020 \\
Punctuation detection       & 0x00000040 \\
Dictionary search           & 0x00000080 \\
Suffixation rules           & 0x00000100 \\
Multiword detection         & 0x00000200 \\
Named entity detection      & 0x00000400 \\
Probability assignment      & 0x00000800 \\
Quantities detection        & 0x00001000 \\
Named entity classification & 0x00002000 \\
Automata (abstract)         & 0x00004000 \\
PoS Tagger (abstract)       & 0x00008000 \\
HMM tagger                  & 0x00010000 \\
Relaxation labelling        & 0x00020000 \\
RL tagger                   & 0x00040000 \\
RL tagger constr. grammar   & 0x00080000 \\
Sense annotation            & 0x00100000 \\
Chart parser                & 0x00200000 \\
Parser grammar              & 0x00400000 \\
Dependency parser           & 0x00800000 \\
Correference resolution     & 0x01000000 \\
Utilities                   & 0x02000000 \\ \hline
\end{tabular}


\item {\bf Language of input text}

\begin{tabular}{|l|l|}
Command line       & Configuration file   \\ \hline
\verb#--lang <language>#  & \verb#Lang=<language>#    \\ \hline
\end{tabular}

   Language of input text (es: Spanish, ca: Catalan, en: English, cy:
   Welsh, it: Italian, gl: Galician, pt: Portuguese).

   Other languages may be added to the library. See
   chapter~\ref{c-adding-lang} for details.

\item {\bf Splitter Buffer Flushing}

\begin{tabular}{|l|l|}
Command line                      & Configuration file   \\ \hline
\verb#--flush#, \verb#--noflush#  & \verb#AlwaysFlush=(yes|y|on|no|n|off)#   \\ \hline
\end{tabular}

   When inactive (most usual choice) sentence splitter buffers lines
   until a sentence marker is found. Then, it outputs a complete
   sentence. 
   When active, the splitter never buffers any token, and 
   considers each newline as sentence end, thus processing each line
   as an independent sentence.

\item {\bf Input Format}

\begin{tabular}{|l|l|}
Command line       & Configuration file   \\ \hline
\verb#--inpf <string>#  & \verb#InputFormat=<string># \\ \hline
\end{tabular}

  Format of input data (plain, token, splitted, morfo, tagged, sense).
 \begin{itemize}
  \item plain: plain text.
  \item token: tokenized text (one token per line).
  \item splitted : tokenized and sentence-splitted text (one token per line, sentences separated with one blank line).
  \item morfo: tokenized, sentence-splitted, and morphologically analyzed text. 
    One token per line, sentences separated with one blank line.\\
    Each line has the format:     {\tt word (lemma tag prob)$^+$ }
  \item tagged:  tokenized, sentence-splitted, morphologically analyzed, and PoS-tagged text. 
    One token per line, sentences separated with one blank line.\\
    Each line has the format:  {\tt word lemma tag}.
  \item sense:  tokenized, sentence-splitted, morphologically
    analyzed, PoS-tagged text, and sense-annotated. 
    One token per line, sentences separated with one blank line.\\
    Each line has the format: 
    {\tt word (lemma tag prob sense$_1$:\ldots:sense$_N$)$^+$}
 \end{itemize}

\item {\bf Output Format}

\begin{tabular}{|l|l|}
Command line       & Configuration file   \\ \hline
\verb#--outf <string>#  & \verb#OutputFormat=<string># \\ \hline
\end{tabular}

  Format of output data (token, splitted, morfo, tagged, parsed, dep).
 \begin{itemize}
  \item token: tokenized text (one token per line).
  \item splitted : tokenized and sentence-splitted text (one token per line, sentences separated with one blank line).
  \item morfo: tokenized, sentence-splitted, and morphologically analyzed text. 
    One token per line, sentences separated with one blank line.\\
    Each line has the format:  \\
    {\tt word (lemma tag prob)$^+$ }\\
    or (if sense tagging has been activated):\\
    {\tt word (lemma tag prob sense$_1$:\ldots:sense$_N$)$^+$}
  \item tagged:  tokenized, sentence-splitted, morphologically analyzed, and PoS-tagged text. 
    One token per line, sentences separated with one blank line.\\
    Each line has the format:  {\tt word lemma tag prob}\\
    or, if sense tagging has been activated: {\tt word lemma tag prob sense$_1$:\ldots:sense$_N$} 
  \item parsed:  tokenized, sentence-splitted, morphologically
  analyzed, PoS-tagged, optionally sense--annotated, and parsed text. 
  \item dep:  tokenized, sentence-splitted, morphologically analyzed, PoS-tagged, optionally sense--annotated, and dependency-parsed text. 
 \end{itemize}

\item {\bf Tokenizer File}

\begin{tabular}{|l|l|}
Command line       & Configuration file   \\ \hline
\verb#--abrev <filename># & \verb#TokenizerFile=<filename>#  \\ \hline
\end{tabular}

  File of tokenization rules. See section~\ref{file-tok} for details.

\item {\bf Splitter File}

\begin{tabular}{|l|l|}
Command line       & Configuration file   \\ \hline
\verb#--fsplit <filename># & \verb#SplitterFile=<filename>#  \\ \hline
\end{tabular}

  File of splitter options rules. See section~\ref{file-split} for details.

\item {\bf Affix Analysis}

\begin{tabular}{|l|l|}
Command line                     & Configuration file   \\ \hline
\verb#--afx#, \verb#--noafx#   & \verb#AffixAnalysis=(yes|y|on|no|n|off)# \\ \hline
\end{tabular}

  Whether to perform affix analysis on unknown words. 
  Affix analysis applies a set of  affixation rules to the word to check whether it is a derived form of a known word.

\item {\bf Affixation Rules File}

\begin{tabular}{|l|l|}
Command line       & Configuration file   \\ \hline
\verb#-S <filename>#, \verb#--fafx <filename>#  & \verb#AffixFile=<filename># \\ \hline  
\end{tabular}

   Affix rules file. See section \ref{file-suf} for details.


\item {\bf Multiword Detection}

\begin{tabular}{|l|l|}
Command line       & Configuration file   \\ \hline
\verb#--loc#, \verb#--noloc#     & \verb#MultiwordsDetection=(yes|y|on|no|n|off)#  \\ \hline 
\end{tabular}

 Whether to perform multiword detection. Multiwords may be detected if a multiword file is provided.
 Multiword File option, below).

\item {\bf Multiword File}

\begin{tabular}{|l|l|}
Command line       & Configuration file   \\ \hline
\verb#-L <filename>#, \verb#--floc <filename># & \verb#LocutionsFile=<filename>#  \\ \hline
\end{tabular}

  Multiword definition file. See section \ref{file-mw} for details.

\item {\bf Number Detection}

\begin{tabular}{|l|l|}
Command line       & Configuration file   \\ \hline
\verb#--numb#, \verb#--nonumb#   & \verb#NumbersDetection=(yes|y|on|no|n|off)# \\ \hline
\end{tabular}

 Whether to perform nummerical expression detection. Deactivating this
 feature will affect the behaviour of date/time and ratio/currency detection modules.

\item {\bf Decimal Point}

\begin{tabular}{|l|l|}
Command line       & Configuration file   \\ \hline
\verb#--dec <string>#   & \verb#DecimalPoint=<string>#\\ \hline 
\end{tabular}

   Specify decimal point character for the number detection module (for instance, in English is a dot, but in Spanish is a comma).

\item {\bf Thousand Point} 

\begin{tabular}{|l|l|}
Command line       & Configuration file   \\ \hline
\verb#--thou <string>#  & \verb#ThousandPoint=<string>#  \\ \hline
\end{tabular}

   Specify thousand point character for the number detection module (for instance, in English is a comma, but in Spanish is a dot).

\item {\bf Punctuation Detection}

\begin{tabular}{|l|l|}
Command line       & Configuration file   \\ \hline
\verb#--punt#, \verb#--nopunt#    & \verb#PunctuationDetection=(yes|y|on|no|n|off)# \\ \hline
\end{tabular}

 Whether to assign PoS tag to punctuation signs.

\item {\bf Punctuation Detection File}

\begin{tabular}{|l|l|}
Command line       & Configuration file   \\ \hline
\verb#-M <filename>#, \verb#--fpunct <filename># & \verb#PunctuationFile=<filename># \\ \hline
\end{tabular}

  Punctuation symbols file.  See section \ref{file-punt} for details.

\item {\bf Date Detection}

\begin{tabular}{|l|l|}
Command line       & Configuration file   \\ \hline
\verb#--date#, \verb#--nodate#    & \verb#DatesDetection=(yes|y|on|no|n|off)# \\ \hline
\end{tabular}

  Whether to perform date and time expression detection.

\item {\bf Quantities Detection}

\begin{tabular}{|l|l|}
Command line       & Configuration file   \\ \hline
\verb#--quant#, \verb#--noquant#   & \verb#QuantitiesDetection=(yes|y|on|no|n|off)# \\ \hline
\end{tabular}

 Whether to perform currency amounts, physical magnitudes, and ratio detection. 

\item {\bf Quantity Recognition File}

\begin{tabular}{|l|l|}
Command line       & Configuration file   \\ \hline
\verb#-Q <filename>#, \verb#--fqty <filename>#  & \verb#QuantitiesFile=<filename>#  \\ \hline
\end{tabular}

 Quantitiy recognition configuration file. See section \ref{file-quant} for details.

\item {\bf Dictionary Search}

\begin{tabular}{|l|l|}
Command line       & Configuration file   \\ \hline
\verb#--dict#, \verb#--nodict#    & \verb#DictionarySearch=(yes|y|on|no|n|off)#    \\ \hline
\end{tabular}

 Whether to search word forms in dictionary. Deactivating this feature
 also deactivates AffixAnalysis option.

\item {\bf Dictionary File}

\begin{tabular}{|l|l|}
Command line       & Configuration file   \\ \hline
\verb#-D <filename>#, \verb#--fdict <filename># & \verb#DictionaryFile=<filename>#  \\ \hline
\end{tabular}

 Dictionary database. Must be a Berkeley DB indexed file. 
 See section \ref{file-dict} and chapter \ref{c-adding-lang} for details.

\item {\bf Probability Assignment}

\begin{tabular}{|l|l|}
Command line       & Configuration file   \\ \hline
\verb#--prob#, \verb#--noprob#    & \verb#ProbabilityAssignment=(yes|y|on|no|n|off)#  \\ \hline
\end{tabular}

 Whether to compute a lexical probability for each tag of each word. 
 Deactivating this feature will affect the behaviour of the PoS tagger.

\item {\bf Lexical Probabilities File}

\begin{tabular}{|l|l|}
Command line       & Configuration file   \\ \hline
\verb#-P <filename>#, \verb#--fprob <filename># & \verb#ProbabilityFile=<filename># \\ \hline
\end{tabular}

 Lexical probabilities file. The probabilities in this file are used
 to compute the most likely tag for a word, as well to estimate the
 likely tags for unknown words. See section \ref{file-prob} for details.

\item {\bf Unknown Words Probability Threshold.}

\begin{tabular}{|l|l|}
Command line       & Configuration file   \\ \hline
\verb#--thres <float>#   & \verb#ProbabilityThreshold=<float>#   \\ \hline   
\end{tabular}

 Threshold that must be reached by the probability of a tag given the
 suffix of an unknown word in order to be included in the list of
 possible tags for that word. Default is zero (all tags are included
 in the list). A non--zero value (e.g. 0.0001, 0.001) is recommended.


\item {\bf Named Entity Recognition}

\begin{tabular}{|l|l|}
Command line       & Configuration file   \\ \hline
\verb#--ner [bio|basic|none]#   & \verb#NERecognition=(bio|basic|none)#    \\ \hline
\end{tabular}

  Whether to perform NE recognition and which recognizer to use:
  ``bio'' for AdaBoost based NER, ``basic'' for a simple heuristic NE
  recognizer and ``none'' to perform no NE recognition . Deactivating
  this feature will cause the NE Classification module to have no
  effect.

\item {\bf Named Entity Recognizer File}

\begin{tabular}{|l|l|}
Command line       & Configuration file   \\ \hline
\verb#--ner [bio|basic|none]#, \verb#--fnp <filename>#   & \verb#NPDataFile=<filename>#  \\ \hline
\end{tabular}

  Configuration data file for active NE recognizer (either ``bio'' or ``basic'').
  See section \ref{file-ner} for details.


\item {\bf Named Entity Classification}

\begin{tabular}{|l|l|}
Command line       & Configuration file   \\ \hline
\verb#--nec#, \verb#--nonec#     & \verb#NEClassification=(yes|y|on|no|n|off)#    \\ \hline
\end{tabular}

   Whether to perform NE classification.


\item {\bf Named Entity Classifier File Prefix}

\begin{tabular}{|l|l|}
Command line       & Configuration file   \\ \hline
\verb#--fnec <filename>#   & \verb#NECFilePrefix=<filename>#  \\ \hline
\end{tabular}

  Prefix to find files for Named Entity Classifier configuration. 

  The searched files will be the given prefix with the following
  extensions: 
 \begin{itemize}
   \item {\tt .rfg}: Feature extractor rule file. 
   \item {\tt .lex}: Feature dictionary.
   \item {\tt .abm}: AdaBoost model for NEC.
 \end{itemize}

  See section \ref{file-nec} for details.

\item {\bf Sense Annotation}

\begin{tabular}{|l|l|}
Command line       & Configuration file   \\ \hline
\verb#--sense <string>#    & \verb#SenseAnnotation=<string>#    \\ \hline
\end{tabular}

   Kind of sense annotation to perform
 \begin{itemize}
  \item no, none: Deactivate sense annotation.
  \item all: annotate with all possible senses in sense dictionary.
  \item mfs: annotate with most frequent sense.
 \end{itemize}

   Whether to perform sense anotation. If active, the PoS tag selected
   by the tagger for each word is enriched with a list of all its
   possible WN synsets. The sense repository used depends on the contents
   of the ``Sense Dictionary File'' described below.

\item {\bf Sense Dictionary File}

\begin{tabular}{|l|l|}
Command line       & Configuration file   \\ \hline
\verb#--fsense <filename>#   & \verb#SenseFile=<filename>#  \\ \hline
\end{tabular}

  Word sense data file. It is a Berkeley DB indexed file.
  See section \ref{file-sense} for details.

\item {\bf Duplicate Analysis for each Sense}

\begin{tabular}{|l|l|}
Command line       & Configuration file   \\ \hline
\verb#--dup#, \verb#--nodup#   & \verb#DuplicateAnalysis=(yes|y|on|no|n|off)#  \\ \hline
\end{tabular}

  When this option is set, the senses annotator will duplicate the
  analysis once for each of its possible senses. 
  See section \ref{mod-sense} for details.

  This may be useful if one wants to perform WSD, or to use the {\em
    sense} field in the {\tt analysis} in the constraint grammar (see
  section \ref{file-relax}).


\item {\bf Tagger algorithm}

\begin{tabular}{|l|l|}
Command line       & Configuration file   \\ \hline
\verb#-T <string>#, \verb#--tag <string>#   & \verb#Tagger=<string>#  \\ \hline
\end{tabular}

   Algorithm to use for PoS tagging
 \begin{itemize}
  \item hmm:  Hidden Markov Model tagger, based on \cite{brants00}.
  \item relax: Relaxation Labelling tagger, based on \cite{padro98a}.
 \end{itemize}

\item {\bf HMM Tagger configuration File}

\begin{tabular}{|l|l|}
Command line       & Configuration file   \\ \hline
\verb#-H <filename>#, \verb#--hmm <filename>#   & \verb#TaggerHMMFile=<filename>#  \\ \hline
\end{tabular}

  Parameters file for HMM tagger. 
  See section \ref{file-hmm} for details.

\item {\bf Relaxation labelling tagger constraints file}

\begin{tabular}{|l|l|}
Command line       & Configuration file   \\ \hline
\verb#-R <filename>#   & \verb#TaggerRelaxFile=<filename>#  \\ \hline
\end{tabular}

   File containing the constraints to apply to solve the PoS tagging.
   See section \ref{file-relax} for details.

\item {\bf Relaxation labelling tagger iteration limit}

\begin{tabular}{|l|l|}
Command line       & Configuration file   \\ \hline
\verb#--iter <int>#   & \verb#TaggerRelaxMaxIter=<int>#  \\ \hline
\end{tabular}

   Maximum numbers of iterations to perform in case relaxation does
   not converge.

\item {\bf Relaxation labelling tagger scale factor}

\begin{tabular}{|l|l|}
Command line       & Configuration file   \\ \hline
\verb#--sf <float>#   & \verb#TaggerRelaxScaleFactor=<float>#  \\ \hline
\end{tabular}

   Scale factor to normalize supports inside RL algorithm. It is
   comparable to the step lenght in a hill-climbing algorithm: The
   larger scale factor, the smaller step.

\item {\bf Relaxation labelling tagger epsilon value}

\begin{tabular}{|l|l|}
Command line       & Configuration file   \\ \hline
\verb#--eps <float>#   & \verb#TaggerRelaxEpsilon=<float>#  \\ \hline
\end{tabular}

   Real value used to determine when a relaxation labelling iteration
   has produced no significant changes. The algorithm stops when no
   weight has changed above the specified epsilon.


\item {\bf Retokenize after tagging}

\begin{tabular}{|l|l|}
Command line       & Configuration file   \\ \hline
\verb#--retk#, \verb#--noretk#    & \verb#TaggerRetokenize=(yes|y|on|no|n|off)#    \\ \hline
\end{tabular}

   Determine whether the tagger must perform retokenization after the
   appropriate analysis has been selected for each word.  This is
   closely related to affix analysis and PoS taggers, see sections
   \ref{file-suf} and \ref{sec-pos} for details.


\item {\bf Force the selection of one unique tag}

\begin{tabular}{|l|l|}
Command line       & Configuration file   \\ \hline
\verb#--force <string>#     & \verb#TaggerForceSelect=(none,tagger,retok)#    \\ \hline
\end{tabular}

   Determine whether the tagger must be forced to (probably randomly) make a unique choice and when.
   \begin{itemize}
    \item   {\tt none}: Do not force the tagger, allow ambiguous output.
    \item {\tt tagger}: Force the tagger to choose before
      retokenization (i.e. if retokenization introduces any ambiguity,
      it will be present in the final output).
    \item   {\tt retok}: Force the tagger to choose after retokenization (no remaining ambiguity)
   \end{itemize}

 See \ref{sec-pos} for more information.

\item {\bf Chart Parser Grammar File}

\begin{tabular}{|l|l|}
Command line       & Configuration file   \\ \hline
\verb#-G <filename>#, \verb#--grammar <filename>#   & \verb#GrammarFile=<filename>#  \\ \hline
\end{tabular}

   This file contains a CFG grammar for the chart parser, and some
  directives to control which chart edges are selected to build the
  final tree.
   See section \ref{file-cfg} for details.


\item {\bf Dependency Parser Rule File}

\begin{tabular}{|l|l|}
Command line       & Configuration file   \\ \hline
\verb#-J <filename>#, \verb#--dep <filename>#   & \verb#DepRulesFile==<filename>#  \\ \hline
\end{tabular}

  Rules to be used to perform dependency analysis.
  See section \ref{file-dep} for details.

\end{itemize}


\newpage
%······························································
\subsection{Sample Configuration File}
\label{ss-config}

  A sample configuration file follows.  This is only a sample, and
  probably won't work if you use it as is.  You can start using
  freeling with the default configuration files which --after
  installation-- are located in {\tt /usr/local/share/FreeLing/config}
  (note than prefix {\tt /usr/local} may differ if you specified an
  alternative location when installing FreeLing).

  You also can use those files as a starting point to customize 
  one configuration file to suit your needs.

  Note that file paths in the sample configuration file contain
  \verb#$FREELINGSHARE#, which is supposed to be an environment
  variable.  If this variable is not defined, the analyzer will
  abort, complaining about not finding the files.

  If you use the {\tt analyze} script, it will define the variable for
  you as {\tt /usr/local/share/Freeling} (or the right installation
  path), unless you define it to point somewhere else.

  You can also adjust your configuration files to use normal paths for
  the files (either relative or absolute) instead of using variables.

{\small
\begin{verbatim}
# ---- sample configuration file for Spanish analyzer

#### General options 
Lang=es

#### Trace options. Only effective if we have compiled with -DVERBOSE
TraceLevel=0
TraceModule=0x0000

## Options to control the applied modules. The input may be partially
## processed, or not a full analysis may me wanted. The specific 
## formats are a choice of the main program using the library, as well
## as the responsability of calling only the required modules.
InputFormat=plain
OutputFormat=morfo

# consider each newline as a sentence end
AlwaysFlush=no

#### Tokenizer options
TokenizerFile="$FREELINGSHARE/es/tokenizer.dat"

#### Splitter options
SplitterFile="$FREELINGSHARE/es/splitter.dat"

#### Morfo options
AffixAnalysis=yes
MultiwordsDetection=yes
NumbersDetection=yes
PunctuationDetection=yes
DatesDetection=yes
QuantitiesDetection=yes
DictionarySearch=yes
ProbabilityAssignment=yes
DecimalPoint=","
ThousandPoint="."
LocutionsFile=$FREELINGSHARE/es/locucions.dat 
QuantitiesFile=$FREELINGSHARE/es/quantities.dat
AffixFile=$FREELINGSHARE/es/afixos.dat
ProbabilityFile=$FREELINGSHARE/es/probabilitats.dat
DictionaryFile=$FREELINGSHARE/es/maco.db
PunctuationFile=$FREELINGSHARE/common/punct.dat
ProbabilityThreshold=0.001
#NER options
NERecognition=basic
NPDataFile=$FREELINGSHARE/es/np.dat
## --- comment lines above and uncomment those below, if you want 
## --- a better NE recognizer (higer accuracy, lower speed)
#NERecognition=bio
#NPDataFile=$FREELINGSHARE/es/ner/ner.dat

## NEC options
NEClassification=no
NECFilePrefix=$FREELINGSHARE/es/nec/nec

## Sense annotation options (none,all,mfs)
SenseAnnotation=none
SenseFile=$FREELINGSHARE/es/senses16.db
DuplicateAnalysis=false

#### Tagger options
Tagger=hmm
#Tagger=relax
TaggerHMMFile=$FREELINGSHARE/es/tagger.dat
TaggerRelaxFile=$FREELINGSHARE/es/constr_gram.dat
TaggerRelaxMaxIter=500
TaggerRelaxScaleFactor=670.0
TaggerRelaxEpsilon=0.001
TaggerRetokenize=yes
TaggerForceSelect=retok

#### Parser options
GrammarFile=$FREELINGSHARE/es/grammar-dep.dat

#### Dependence Parser options
DepParser=txala
DepTxalaFile=$FREELINGSHARE/es/dep/dependences.dat
DepMaltFile=$FREELINGSHARE/es/malt/malt.dat

#### Coreference Solver options
CoreferenceResolution=no
CorefFile=$FREELINGSHARE/es/coref/coref.dat
\end{verbatim}
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5
\chapter{Extending the library with analyzers for new languages}
\label{c-adding-lang}
 
  It is possible to extend the library with capability to deal with a
  new language. In some cases, this may be done without reprogramming,
  but for accurate results, some modules would require entering into
  the code.

  Since the text input language is an configuration option of the
  system, a new configuration file must be created for the language to
  be added (e.g. copying and modifying an existing one, such as the example
  presented in section~\ref{ss-config}). 
 
 %..................................................
 \section{Tokenizer}
  The first module in the processing chain is the tokenizer. As
  described in section~\ref{ss-options}, the behaviour of the
  tokenizer is controlled via the TokenizerFile option in
  configuration file. 

  To create a tokenizer for a new language, just create a new
  tokenization rules file (e.g. copying an existing one and adapting 
  its regexps to particularities of your language), and set 
  it as the value for the TokenizerFile option in your new 
  configuration file.

%..................................................
  \section{Morphological analyzer}
   The morphological analyzer module consists of several sub-modules
   that may require language customization. See
   sections~\ref{sec-maco} to \ref{file-prob} for details on data file formats for each
   module:

   \subsection{Multiword detection} 
    The LocutionsFile option in
    configuration file must be set to the name of a file that contains
    the multiwords you want to detect in your language. 

   \subsection{Nummerical expression detection} 
    If no specialized module is defined to detect nummerical
    expressions, the default behaviour is to recognize only numbers
    and codes written in digits (or mixing digits and non-digit
    characters).
  
    If you want to recognize language dependent expressions (such as
    numbers expressed in words --e.g. ``one hundred thirthy-six''),
    you have to program a {\em numbers\_mylanguage} class derived from
    abstract class {\em numbers\_module}.  Those classes are finite
    automata that recognize word sequences. An abstract class {\em
    automat} controls the sequence advance, so your derived class has
    little work to do apart from defining states and transitions for
    the automaton.

    A good idea to start with this issue is having a look at the 
    {\em numbers\_es}, {\em numbers\_en}, and {\em numbers\_ca} classes.
    State/transition diagrams of those automata can be found in the
    directory {\tt doc/diagrams}.
 
   \subsection{Date/time expression detection} 
    If no specialized module is defined to detect date/time
    expressions, the default behaviour is to recognize only simple
    date expressions (such as DD/MM/YYYY).
  
    If you want to recognize language dependent expressions (such as
    complex time expressions --e.g. ``wednesday, July 12th at half
    past nine''), you have to program a {\em date\_mylanguage} class
    derived from abstract class {\em dates\_module}.  Those classes
    are finite automata that recognize word sequences. An abstract
    class {\em automat} controls the sequence advance, so your derived
    class has little work to do apart from defining states and
    transitions for the automaton.

    A good idea to start with this issue is having a look at the 
    {\em dates\_es}, {\em dates\_en}, and {\em dates\_ca} classes.
    State/transition diagrams of those automata can be found in the
    directory {\tt doc/diagrams}.

   \subsection{Currency/ratio expression detection} If no specialized
    module is defined to detect date/time expressions, the default
    behaviour is to recognize only simple percentage expressions (such
    as ``23\%'').
  
    If you want to recognize language dependent expressions (such as
    complex ratio expressions --e.g. ``three out of four''-- or
    currency expression --e.g. ``2,000 australian dollar''), you have
    to program a {\em quantities\_mylanguage} class derived from
    abstract class {\em quantities\_module}.  Those classes are finite
    automata that recognize word sequences. An abstract class {\em
    automat} controls the sequence advance, so your derived class has
    little work to do apart from defining states and transitions for
    the automaton.

    A good idea to start with this issue is having a look at the {\em
    quantities\_es} and {\em quantities\_ca}
    classes.  

    In the case your language is a roman language (or at least, has a
    similar structure for currency expressions) you can easily develop
    your currency expression detector by copying the {\em quantities\_es}
    class, and modifying the CurrencyFile option to provide a file in
    which lexical items are adapted to your language.
    For instance: Catalan currency recognizer uses a copy of the 
    {\em quantities\_es} class, but a different CurrencyFile, since
    the syntactical structure for currency expression is the same in
    both languages, but lexical forms are different.

    If your language has a very different structure for those
    expressions, you may require a different format for the 
    CurrencyFile contents. Since that file will be used only 
    for your language, feel free to readjust its format.

   \subsection{Dictionary search} 
    The lexical forms for each language are sought in a Berkeley
    Database. You only have to specify in which file it is found
    with the DictionaryFile option.

    The dictionary file can be build with the {\tt indexdict} program
    you'll find in the binaries directory of FreeLing. This program
    reads data from stdin and indexes them into a DB file with the
    name given as a parameter.
   
    The input data is expected to contain one word form per line, each line
    with the format: \\
    {\tt form lemma1 tag1 lemma2 tag2 ...}\\
    E.g.\\
    {\tt abalanzaría abalanzar VMIC1S0 abalanzar VMIC3S0\\
    bajo bajar VMIP1S0 bajo AQ0MS0 bajo NCMS000 bajo SPS00\\
    efusivas efusivo AQ0FP0}

   \subsection{Affixed forms search} 
    Forms not found in dictionary may be submitted to an affix
    analysis to devise whether they are derived forms. The valid
    affixes and their application contexts are defined in the 
    affix rule file referred by AffixFile configuration option. 
    See section~\ref{file-suf} for details on affixation rules
    format.
   
     If your language has ortographic accentuation (such as Spanish,
    Catalan, and many other roman languages), the suffixation rules
    may have to deal with accent restoration when rebuilding the
    original roots. To do this, you have to to program a {\em
    accents\_mylanguage} class derived from abstract class {\em
    accents\_module}, which provides the service of restoring
    (according to the accentuation rules in your languages)
    accentuation in a root obtained after removing a given suffix.

    A good idea to start with this issue is having a look at the 
    {\em accents\_es} class.

   \subsection{Probability assignment} 

     The module in charge of assigning lexical probabilities to each
    word analysis only requires a data file, referenced by the 
    ProbabilityFile configuration option. 

     This file may be creted using a tagged corpus and the script
   provided in {\tt src/utilities/TRAIN}.

    See section~\ref{file-prob} for format details.

%..................................................
  \section{HMM PoS Tagger}

   The HMM PoS tagger only requires an appropriate HMM parameters file, 
   given by the TaggerHMMFile option. See section~\ref{file-hmm}
   for format details.
 
   To build a HMM tagger for a new language, you will need corpus 
   (preferably tagged), and you will have to write some probability 
   estimation scripts (e.g. you may use MLE with a simple add-one 
   smoothing).

   Nevertheless, the easiest way (if you have a tagged corpus) is
   using the estimation and smoothing script {\tt
   src/utilities/TRAIN} provided in FreeLing package.

%..................................................
  \section{Relaxation Labelling PoS Tagger}

   The Relaxation Labelling PoS tagger only requires an appropriate
   pseudo- constraint grammar file,  given by the RelaxTaggerFile
   option. See section~\ref{file-relax} for format details.

   To build a Relax tagger for a new language, you will need corpus (preferably
   tagged), and you will have to write some compatibility estimation
   scripts. You can also write from scratch a knowledge-based constraint grammar.

   Nevertheless, the easiest way (if you have an annotated corpus) is
   using the estimation and smoothing script {\tt
   src/utilities/TRAIN} provided in FreeLing package.  

   The produced constraint grammar files contain only simple bigram
   constraints, but the model may be improved by hand coding more
   complex context constraint, as can be seen in the Spanish data file
   in {\tt share/FreeLing/es/constr\_grammar.dat}

%..................................................
  \section{Sense annotation}
\label{senses-file}
  The sense annotation module uses a BerkeleyDB indexed file. This
  file may also be used by the dependency labeling module (see section
  \ref{file-dep}).
 
  It may be created for a new language (or or form a new knowledge source) with the
  program
  {\tt src/utilities/indexdict} provided with FreeLing. 

  The source file must have the sense list for one lemma--PoS at each line.\\

  Each line has format: {\tt W:lemma:PoS synset1 synset2 ...}.  \\
  E.g. \\
  \verb#W:cebolla:N 05760066 08734429 08734702#

  The first sense code in the list is assumed to be the most frequent
  sense for that lemma--PoS by the sense annotation module.
  This only takes effect when value {\tt msf} is selected for the {\tt SenseAnnotation} option. 

  The file may also contain the same information indexed by synset
  (that is, the list of synonyms for a given synset). This is needed
  if you are using the \verb#synon# function in your dependency rules
  (see section \ref{file-dep}), or if you want to access that
  information for your own purposes.  The lines with this information
  have the format:

  Each line has format: {\tt S:synset:PoS lemma1 lemma2 ...}.  E.g. \\
   \verb#S:07389783:N chaval chico joven mozo muchacho niño#

  To create a sense file for a new language, just list the sense codes
  for each lemma-PoS combination in a text file
  (e.g. \verb#sensefile.txt#), with lines in the format described above, and
  then issue:\\
  \verb#indexdict sense.db < sensefile.txt#

   This will produce an indexed file {\tt sense.db} which is to be
   given to the analyzer via the {\tt SenseFile} option in
   configuration file, or via the {\tt --fsense} option at command
   line.  It can also be referred to in the entry {\tt WNFile} of the
   \verb#<SEMDB># section of a file of dependency labeling rules
   (section \ref{file-dep}).

%..................................................
  \section{Chart Parser}

   The parser only requires a grammar which is consistent with the
   tagset used in the morphological and tagging steps.
   The grammar file must be specified in the GrammarFile option
   (or passed to the parser constructor). See section~\ref{file-cfg}
   for format details.

%..................................................
  \section{Dependency Parser}

   The depencency parser only requires a set of rules which is consistent with the
   PoS tagset and the non-terminal categories generated by the Chart
   Parser grammar.
   The grammar file must be specified in the DepRulesFile option
   (or passed to the parser constructor). See section~\ref{file-dep}
   for format details.

\bibliographystyle{alpha}
\bibliography{biblio} 

\end{document}

